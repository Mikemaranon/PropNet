{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ocultar advertencias de TensorFlow y otros warnings innecesarios\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Oculta logs de TensorFlow (0 = todos, 1 = INFO, 2 = WARNING, 3 = ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Oculta warnings de Python\n",
    "\n",
    "# Desactivar ejecución ansiosa (si no la necesitas)\n",
    "tf.config.run_functions_eagerly(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   parking  area  prefarea  unfurnished  airconditioning  stories  mainroad  \\\n",
      "0        2  7420         1            0                1        3         1   \n",
      "1        3  8960         0            0                1        4         1   \n",
      "2        2  9960         1            0                0        2         1   \n",
      "3        3  7500         1            0                1        2         1   \n",
      "4        2  7420         0            0                1        2         1   \n",
      "\n",
      "      price  basement  semi-furnished  ...  KPI_Mainroad_Area_Ratio  \\\n",
      "0  13300000         0               0  ...                     7420   \n",
      "1  12250000         0               0  ...                     8960   \n",
      "2  12250000         1               1  ...                     9960   \n",
      "3  12215000         1               0  ...                     7500   \n",
      "4  11410000         1               0  ...                     7420   \n",
      "\n",
      "   KPI_Stories_per_Land_Area  KPI_Bedrooms_AirConditioning_Ratio  \\\n",
      "0                   0.000404                                0.25   \n",
      "1                   0.000446                                0.25   \n",
      "2                   0.000201                                0.00   \n",
      "3                   0.000267                                0.25   \n",
      "4                   0.000270                                0.25   \n",
      "\n",
      "   KPI_Area_per_SemiFurnished  KPI_Bathrooms_Area_Ratio  \\\n",
      "0                         inf                  0.000270   \n",
      "1                         inf                  0.000446   \n",
      "2                      9960.0                  0.000201   \n",
      "3                         inf                  0.000267   \n",
      "4                         inf                  0.000135   \n",
      "\n",
      "   KPI_Bathrooms_per_Furnishing  KPI_Area_per_Guestroom  \\\n",
      "0                           2.0                     inf   \n",
      "1                           4.0                     inf   \n",
      "2                           2.0                     inf   \n",
      "3                           2.0                     inf   \n",
      "4                           1.0                  7420.0   \n",
      "\n",
      "   KPI_Bedrooms_per_Guestroom  KPI_Parking_Area_Ratio  KPI_Basement_Area_Ratio  \n",
      "0                         inf                0.000270                 0.000000  \n",
      "1                         inf                0.000335                 0.000000  \n",
      "2                         inf                0.000201                 0.000100  \n",
      "3                         inf                0.000400                 0.000133  \n",
      "4                         4.0                0.000270                 0.000135  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Number of records (rows): 3045\n",
      "Number of columns: 42\n",
      "\n",
      "Column names:\n",
      "['parking', 'area', 'prefarea', 'unfurnished', 'airconditioning', 'stories', 'mainroad', 'price', 'basement', 'semi-furnished', 'furnished', 'bathrooms', 'bedrooms', 'guestroom', 'KPI_Area_per_Bedroom', 'KPI_Bathrooms_per_Bedroom', 'KPI_Area_per_Story', 'KPI_AirConditioning_Presence', 'KPI_Bathrooms_per_Area', 'KPI_Area_per_Furnishing', 'KPI_Parking_in_Prefarea', 'KPI_Area_per_Parking', 'KPI_Prefarea_Presence', 'KPI_Area_per_Bathroom', 'KPI_Bedrooms_per_Area', 'KPI_Stories_per_Area', 'KPI_AirConditioning_Area_Ratio', 'KPI_Area_per_Bedroom_Bathroom', 'KPI_Bedrooms_per_Bathroom', 'KPI_Furnished_Area_Ratio', 'KPI_SemiFurnished_Area_Ratio', 'KPI_Stories_per_Bathroom', 'KPI_Mainroad_Area_Ratio', 'KPI_Stories_per_Land_Area', 'KPI_Bedrooms_AirConditioning_Ratio', 'KPI_Area_per_SemiFurnished', 'KPI_Bathrooms_Area_Ratio', 'KPI_Bathrooms_per_Furnishing', 'KPI_Area_per_Guestroom', 'KPI_Bedrooms_per_Guestroom', 'KPI_Parking_Area_Ratio', 'KPI_Basement_Area_Ratio']\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/mike/Escritorio/codes/projects/PropNet/PropNet-project/2_Data_Processing/feature_engineering/processed_data.csv\"\n",
    "data = pd.read_csv(path)\n",
    "print(data.head())\n",
    "\n",
    "# Get number of rows and columns\n",
    "num_rows, num_cols = data.shape\n",
    "\n",
    "print(f\"Number of records (rows): {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\")\n",
    "\n",
    "# Optional: Display column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['price'])  # Todas las columnas excepto 'price'\n",
    "y = data['price']  # Precio de la vivienda\n",
    "\n",
    "# First check for and handle infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Then handle NaN values\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "all_scaled_price = price_scaler.fit_transform(y.to_numpy().reshape(-1, 1)) \n",
    "all_scaled_vars = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_scaled_vars, all_scaled_price, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2436, 41)\n",
      "(2436, 1)\n",
      "(609, 41)\n",
      "(609, 1)\n"
     ]
    }
   ],
   "source": [
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# y_train_scaled = scaler.fit_transform(y_train)\n",
    "# y_test_scaled = scaler.transform(y_test)\n",
    "\n",
    "\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "y_train_scaled = y_train\n",
    "y_test_scaled = y_test\n",
    "\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Lion\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def build_model(optimizer_name='SGD'):\n",
    "    optimizers_dict = {\n",
    "        'Adam': Adam(learning_rate=0.001),\n",
    "        'SGD': SGD(learning_rate=0.001, momentum=0.9),\n",
    "        'RMSprop': RMSprop(learning_rate=0.001),\n",
    "        'Adagrad': Adagrad(learning_rate=0.001),\n",
    "        'Adadelta': Adadelta(learning_rate=1.0),\n",
    "        'Adamax': Adamax(learning_rate=0.002),\n",
    "        'Nadam': Nadam(learning_rate=0.001),\n",
    "        'Lion' : Lion(learning_rate=0.001)\n",
    "    }\n",
    "\n",
    "    if optimizer_name not in optimizers_dict:\n",
    "        raise ValueError(f\"Optimizador {optimizer_name} no reconocido. Opciones válidas: {list(optimizers_dict.keys())}\")\n",
    "\n",
    "    optimizer = optimizers_dict[optimizer_name]\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        # layers.Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        # layers.Dropout(0.4),  # Dropout aumentado\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.2),  # Dropout aumentado\n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.2),  # Dropout en cada capa para reducir sobreajuste\n",
    "        layers.Dense(24, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.2),  # Dropout en cada capa\n",
    "        layers.Dense(1)  # Capa de salida\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con el optimizador: Adam\n",
      "Entrenando con el optimizador: SGD\n",
      "Entrenando con el optimizador: Adadelta\n",
      "Entrenando con el optimizador: Lion\n",
      "============================================================\n",
      "\n",
      "Resultados de Validación Cruzada (MAE promedio):\n",
      "Adam: 0.1014\n",
      "SGD: 0.0979\n",
      "Adadelta: 0.1023\n",
      "Lion: 0.1007\n",
      "============================================================\n",
      "\n",
      "Mejor optimizador seleccionado: SGD\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Definir validación cruzada\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lista de optimizadores a probar\n",
    "optimizers = ['Adam', 'SGD', 'Adadelta', 'Lion']\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "results = {}\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "for opt_name in optimizers:\n",
    "    fold_mae = []\n",
    "\n",
    "    print(f\"Entrenando con el optimizador: {opt_name}\")\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(X_train_scaled):\n",
    "        X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train_scaled[train_idx], y_train_scaled[val_idx]\n",
    "\n",
    "        try:\n",
    "            # Crear un nuevo modelo con el optimizador especificado\n",
    "            model = build_model(optimizer_name=opt_name)\n",
    "\n",
    "            # Entrenar el modelo\n",
    "            history = model.fit(X_train_fold, y_train_fold, epochs=30, batch_size=24,\n",
    "                                        validation_data=(X_val_fold, y_val_fold), verbose=0,\n",
    "                                        callbacks=[early_stopping])\n",
    "\n",
    "            # Evaluar el modelo\n",
    "            val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)[1]\n",
    "            fold_mae.append(val_mae)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error con optimizador {opt_name}: {e}\")\n",
    "            continue  # Saltar a la siguiente iteración si hay un error\n",
    "\n",
    "    # Guardar el MAE promedio en los resultados solo si hubo evaluaciones válidas\n",
    "    if fold_mae:\n",
    "        results[opt_name] = np.mean(fold_mae)\n",
    "    else:\n",
    "        results[opt_name] = float('inf')  # Asignar un valor alto si no se pudo entrenar\n",
    "\n",
    "print('============================================================')\n",
    "# Imprimir los resultados de validación cruzada\n",
    "print(\"\\nResultados de Validación Cruzada (MAE promedio):\")\n",
    "for opt, mae in results.items():\n",
    "    print(f\"{opt}: {mae:.4f}\")\n",
    "print('============================================================')\n",
    "# Seleccionar el mejor optimizador\n",
    "best_optimizer_name = min(results, key=results.get)\n",
    "print(f\"\\nMejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "print('============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor optimizador seleccionado: SGD\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Elegir el mejor optimizador basado en el menor MAE promedio\n",
    "best_optimizer_name = min(results, key=results.get)\n",
    "# best_optimizer_name = 'Lion'\n",
    "print(f\"Mejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "\n",
    "# Construir y entrenar el modelo final con el mejor optimizador\n",
    "model = build_model(optimizer_name=best_optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 8.0302 - mae: 0.2337 - val_loss: 7.6051 - val_mae: 0.1136 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4836 - mae: 0.1272 - val_loss: 7.0994 - val_mae: 0.1079 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9803 - mae: 0.1141 - val_loss: 6.6154 - val_mae: 0.1043 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5007 - mae: 0.1103 - val_loss: 6.1523 - val_mae: 0.1031 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0413 - mae: 0.1076 - val_loss: 5.7095 - val_mae: 0.1022 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6028 - mae: 0.1013 - val_loss: 5.2873 - val_mae: 0.1017 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1873 - mae: 0.1047 - val_loss: 4.8843 - val_mae: 0.1014 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7863 - mae: 0.1009 - val_loss: 4.5003 - val_mae: 0.1014 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4073 - mae: 0.1006 - val_loss: 4.1349 - val_mae: 0.1017 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0463 - mae: 0.1012 - val_loss: 3.7877 - val_mae: 0.1020 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7049 - mae: 0.1046 - val_loss: 3.4586 - val_mae: 0.1024 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3790 - mae: 0.1007 - val_loss: 3.1472 - val_mae: 0.1028 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0705 - mae: 0.0993 - val_loss: 2.8522 - val_mae: 0.1032 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7827 - mae: 0.1061 - val_loss: 2.5737 - val_mae: 0.1035 - learning_rate: 0.0010\n",
      "Epoch 15/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5064 - mae: 0.1013 - val_loss: 2.3118 - val_mae: 0.1037 - learning_rate: 0.0010\n",
      "Epoch 16/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2484 - mae: 0.1019 - val_loss: 2.0648 - val_mae: 0.1043 - learning_rate: 0.0010\n",
      "Epoch 17/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0066 - mae: 0.1062 - val_loss: 1.8331 - val_mae: 0.1046 - learning_rate: 0.0010\n",
      "Epoch 18/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7789 - mae: 0.1056 - val_loss: 1.6171 - val_mae: 0.1047 - learning_rate: 0.0010\n",
      "Epoch 19/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5662 - mae: 0.1056 - val_loss: 1.4164 - val_mae: 0.1051 - learning_rate: 0.0010\n",
      "Epoch 20/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3688 - mae: 0.1037 - val_loss: 1.2306 - val_mae: 0.1050 - learning_rate: 0.0010\n",
      "Epoch 21/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1871 - mae: 0.1049 - val_loss: 1.0594 - val_mae: 0.1053 - learning_rate: 0.0010\n",
      "Epoch 22/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0188 - mae: 0.1045 - val_loss: 0.9033 - val_mae: 0.1055 - learning_rate: 0.0010\n",
      "Epoch 23/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8671 - mae: 0.1060 - val_loss: 0.7621 - val_mae: 0.1055 - learning_rate: 0.0010\n",
      "Epoch 24/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7279 - mae: 0.1029 - val_loss: 0.6341 - val_mae: 0.1056 - learning_rate: 0.0010\n",
      "Epoch 25/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6035 - mae: 0.1025 - val_loss: 0.5194 - val_mae: 0.1056 - learning_rate: 0.0010\n",
      "Epoch 26/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4919 - mae: 0.1024 - val_loss: 0.4186 - val_mae: 0.1058 - learning_rate: 0.0010\n",
      "Epoch 27/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3962 - mae: 0.1062 - val_loss: 0.3316 - val_mae: 0.1057 - learning_rate: 0.0010\n",
      "Epoch 28/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3111 - mae: 0.1015 - val_loss: 0.2571 - val_mae: 0.1058 - learning_rate: 0.0010\n",
      "Epoch 29/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2413 - mae: 0.1068 - val_loss: 0.1944 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 30/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1816 - mae: 0.1069 - val_loss: 0.1438 - val_mae: 0.1058 - learning_rate: 0.0010\n",
      "Epoch 31/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1330 - mae: 0.1038 - val_loss: 0.1059 - val_mae: 0.1057 - learning_rate: 0.0010\n",
      "Epoch 32/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0975 - mae: 0.1041 - val_loss: 0.0804 - val_mae: 0.1061 - learning_rate: 0.0010\n",
      "Epoch 33/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0751 - mae: 0.1048 - val_loss: 0.0644 - val_mae: 0.1057 - learning_rate: 0.0010\n",
      "Epoch 34/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0613 - mae: 0.1054 - val_loss: 0.0537 - val_mae: 0.1057 - learning_rate: 0.0010\n",
      "Epoch 35/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503 - mae: 0.1043 - val_loss: 0.0449 - val_mae: 0.1057 - learning_rate: 0.0010\n",
      "Epoch 36/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0411 - mae: 0.1026 - val_loss: 0.0373 - val_mae: 0.1060 - learning_rate: 0.0010\n",
      "Epoch 37/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0339 - mae: 0.1021 - val_loss: 0.0310 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 38/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0285 - mae: 0.1043 - val_loss: 0.0263 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 39/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0240 - mae: 0.1030 - val_loss: 0.0228 - val_mae: 0.1061 - learning_rate: 0.0010\n",
      "Epoch 40/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0220 - mae: 0.1061 - val_loss: 0.0208 - val_mae: 0.1058 - learning_rate: 0.0010\n",
      "Epoch 41/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0203 - mae: 0.1042 - val_loss: 0.0199 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 42/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0194 - mae: 0.1045 - val_loss: 0.0198 - val_mae: 0.1060 - learning_rate: 0.0010\n",
      "Epoch 43/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0179 - mae: 0.1015 - val_loss: 0.0198 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 44/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0204 - mae: 0.1072 - val_loss: 0.0198 - val_mae: 0.1058 - learning_rate: 0.0010\n",
      "Epoch 45/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0175 - mae: 0.1004 - val_loss: 0.0198 - val_mae: 0.1060 - learning_rate: 0.0010\n",
      "Epoch 46/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.0179 - mae: 0.1018\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0180 - mae: 0.1021 - val_loss: 0.0198 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 47/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - mae: 0.1034 - val_loss: 0.0196 - val_mae: 0.1058 - learning_rate: 5.0000e-04\n",
      "Epoch 48/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0195 - mae: 0.1057 - val_loss: 0.0195 - val_mae: 0.1059 - learning_rate: 5.0000e-04\n",
      "Epoch 49/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1062 - val_loss: 0.0196 - val_mae: 0.1058 - learning_rate: 5.0000e-04\n",
      "Epoch 50/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0190 - mae: 0.1049 - val_loss: 0.0195 - val_mae: 0.1058 - learning_rate: 5.0000e-04\n",
      "Epoch 51/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - mae: 0.1052 - val_loss: 0.0195 - val_mae: 0.1059 - learning_rate: 5.0000e-04\n",
      "Epoch 52/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 0.0185 - mae: 0.1033\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0186 - mae: 0.1035 - val_loss: 0.0195 - val_mae: 0.1059 - learning_rate: 5.0000e-04\n",
      "Epoch 53/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0194 - mae: 0.1062 - val_loss: 0.0194 - val_mae: 0.1059 - learning_rate: 2.5000e-04\n",
      "Epoch 54/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - mae: 0.1024 - val_loss: 0.0194 - val_mae: 0.1059 - learning_rate: 2.5000e-04\n",
      "Epoch 55/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0174 - mae: 0.1024 - val_loss: 0.0194 - val_mae: 0.1058 - learning_rate: 2.5000e-04\n",
      "Epoch 56/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.1045 - val_loss: 0.0194 - val_mae: 0.1058 - learning_rate: 2.5000e-04\n",
      "Epoch 57/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mae: 0.1019 - val_loss: 0.0194 - val_mae: 0.1059 - learning_rate: 2.5000e-04\n",
      "Epoch 58/250\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 0.0191 - mae: 0.1030\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1032 - val_loss: 0.0194 - val_mae: 0.1059 - learning_rate: 2.5000e-04\n",
      "Epoch 59/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0179 - mae: 0.1003 - val_loss: 0.0193 - val_mae: 0.1059 - learning_rate: 1.2500e-04\n",
      "Epoch 60/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mae: 0.1033 - val_loss: 0.0193 - val_mae: 0.1059 - learning_rate: 1.2500e-04\n",
      "Epoch 61/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1057 - val_loss: 0.0193 - val_mae: 0.1059 - learning_rate: 1.2500e-04\n",
      "Epoch 62/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1045 - val_loss: 0.0193 - val_mae: 0.1058 - learning_rate: 1.2500e-04\n",
      "Epoch 63/250\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 0.0186 - mae: 0.1039\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0186 - mae: 0.1040 - val_loss: 0.0193 - val_mae: 0.1059 - learning_rate: 1.2500e-04\n",
      "Epoch 64/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1043 - val_loss: 0.0192 - val_mae: 0.1058 - learning_rate: 6.2500e-05\n",
      "Epoch 65/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0190 - mae: 0.1043 - val_loss: 0.0192 - val_mae: 0.1058 - learning_rate: 6.2500e-05\n",
      "Epoch 66/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0197 - mae: 0.1064 - val_loss: 0.0192 - val_mae: 0.1058 - learning_rate: 6.2500e-05\n",
      "Epoch 67/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0196 - mae: 0.1067 - val_loss: 0.0192 - val_mae: 0.1058 - learning_rate: 6.2500e-05\n",
      "Epoch 68/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0199 - mae: 0.1075 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 6.2500e-05\n",
      "Epoch 69/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.0188 - mae: 0.1044\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mae: 0.1044 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 6.2500e-05\n",
      "Epoch 70/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0196 - mae: 0.1067 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.1250e-05\n",
      "Epoch 71/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0187 - mae: 0.1052 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.1250e-05\n",
      "Epoch 72/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - mae: 0.1030 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.1250e-05\n",
      "Epoch 73/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1078 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.1250e-05\n",
      "Epoch 74/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.0199 - mae: 0.1056\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1054 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.1250e-05\n",
      "Epoch 75/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0190 - mae: 0.1051 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5625e-05\n",
      "Epoch 76/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0194 - mae: 0.1066 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5625e-05\n",
      "Epoch 77/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0174 - mae: 0.1021 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5625e-05\n",
      "Epoch 78/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0181 - mae: 0.1036 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5625e-05\n",
      "Epoch 79/250\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 0.0201 - mae: 0.1069\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0199 - mae: 0.1066 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5625e-05\n",
      "Epoch 80/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1063 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.8125e-06\n",
      "Epoch 81/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0171 - mae: 0.1002 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.8125e-06\n",
      "Epoch 82/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1048 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.8125e-06\n",
      "Epoch 83/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0180 - mae: 0.1033 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.8125e-06\n",
      "Epoch 84/250\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 0.0188 - mae: 0.1035\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mae: 0.1037 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.8125e-06\n",
      "Epoch 85/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0200 - mae: 0.1064 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.9063e-06\n",
      "Epoch 86/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0199 - mae: 0.1073 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.9063e-06\n",
      "Epoch 87/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0171 - mae: 0.1000 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.9063e-06\n",
      "Epoch 88/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mae: 0.1017 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.9063e-06\n",
      "Epoch 89/250\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 0.0188 - mae: 0.1049\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mae: 0.1049 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.9063e-06\n",
      "Epoch 90/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0182 - mae: 0.1027 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9531e-06\n",
      "Epoch 91/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0181 - mae: 0.1046 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9531e-06\n",
      "Epoch 92/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0196 - mae: 0.1056 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9531e-06\n",
      "Epoch 93/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mae: 0.1008 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9531e-06\n",
      "Epoch 94/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 0.0198 - mae: 0.1070\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0197 - mae: 0.1067 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9531e-06\n",
      "Epoch 95/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0197 - mae: 0.1071 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.7656e-07\n",
      "Epoch 96/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1048 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.7656e-07\n",
      "Epoch 97/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0195 - mae: 0.1061 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.7656e-07\n",
      "Epoch 98/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0195 - mae: 0.1054 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.7656e-07\n",
      "Epoch 99/250\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 946us/step - loss: 0.0188 - mae: 0.1035\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mae: 0.1036 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.7656e-07\n",
      "Epoch 100/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.1067 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 4.8828e-07\n",
      "Epoch 101/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0179 - mae: 0.1023 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 4.8828e-07\n",
      "Epoch 102/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1056 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 4.8828e-07\n",
      "Epoch 103/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1064 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 4.8828e-07\n",
      "Epoch 104/250\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 0.0199 - mae: 0.1078\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0197 - mae: 0.1073 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 4.8828e-07\n",
      "Epoch 105/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1032 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 2.4414e-07\n",
      "Epoch 106/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - mae: 0.1053 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 2.4414e-07\n",
      "Epoch 107/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - mae: 0.1042 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 2.4414e-07\n",
      "Epoch 108/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0195 - mae: 0.1047 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 2.4414e-07\n",
      "Epoch 109/250\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 0.0189 - mae: 0.1029\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mae: 0.1030 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 2.4414e-07\n",
      "Epoch 110/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1054 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.2207e-07\n",
      "Epoch 111/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1050 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.2207e-07\n",
      "Epoch 112/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.1044 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.2207e-07\n",
      "Epoch 113/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0181 - mae: 0.1034 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.2207e-07\n",
      "Epoch 114/250\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 0.0188 - mae: 0.1058\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mae: 0.1056 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.2207e-07\n",
      "Epoch 115/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0186 - mae: 0.1039 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 6.1035e-08\n",
      "Epoch 116/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0186 - mae: 0.1037 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 6.1035e-08\n",
      "Epoch 117/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1067 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 6.1035e-08\n",
      "Epoch 118/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1058 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 6.1035e-08\n",
      "Epoch 119/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.0173 - mae: 0.1020\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0175 - mae: 0.1022 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 6.1035e-08\n",
      "Epoch 120/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.1064 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.0518e-08\n",
      "Epoch 121/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0185 - mae: 0.1022 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.0518e-08\n",
      "Epoch 122/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mae: 0.1019 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.0518e-08\n",
      "Epoch 123/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0178 - mae: 0.1023 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.0518e-08\n",
      "Epoch 124/250\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 0.0191 - mae: 0.1058\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1057 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.0518e-08\n",
      "Epoch 125/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0173 - mae: 0.1016 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5259e-08\n",
      "Epoch 126/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0196 - mae: 0.1058 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5259e-08\n",
      "Epoch 127/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0183 - mae: 0.1036 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5259e-08\n",
      "Epoch 128/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.1045 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5259e-08\n",
      "Epoch 129/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 0.0194 - mae: 0.1076\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0194 - mae: 0.1073 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.5259e-08\n",
      "Epoch 130/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0184 - mae: 0.1046 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.6294e-09\n",
      "Epoch 131/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mae: 0.1056 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.6294e-09\n",
      "Epoch 132/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0190 - mae: 0.1066 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.6294e-09\n",
      "Epoch 133/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.1080 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.6294e-09\n",
      "Epoch 134/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.0206 - mae: 0.1081\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0204 - mae: 0.1077 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 7.6294e-09\n",
      "Epoch 135/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0190 - mae: 0.1039 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.8147e-09\n",
      "Epoch 136/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0175 - mae: 0.1016 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.8147e-09\n",
      "Epoch 137/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1059 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.8147e-09\n",
      "Epoch 138/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1040 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.8147e-09\n",
      "Epoch 139/250\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 0.0184 - mae: 0.1041\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0184 - mae: 0.1042 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 3.8147e-09\n",
      "Epoch 140/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0183 - mae: 0.1038 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9073e-09\n",
      "Epoch 141/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.1045 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9073e-09\n",
      "Epoch 142/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0180 - mae: 0.1033 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9073e-09\n",
      "Epoch 143/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0183 - mae: 0.1028 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9073e-09\n",
      "Epoch 144/250\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 0.0193 - mae: 0.1045\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0192 - mae: 0.1045 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 1.9073e-09\n",
      "Epoch 145/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0197 - mae: 0.1076 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.5367e-10\n",
      "Epoch 146/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0199 - mae: 0.1076 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.5367e-10\n",
      "Epoch 147/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0180 - mae: 0.1015 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.5367e-10\n",
      "Epoch 148/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0192 - mae: 0.1051 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.5367e-10\n",
      "Epoch 149/250\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 0.0177 - mae: 0.1019\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mae: 0.1021 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 9.5367e-10\n",
      "Epoch 150/250\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - mae: 0.1055 - val_loss: 0.0192 - val_mae: 0.1059 - learning_rate: 4.7684e-10\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=250, batch_size=32,\n",
    "                    validation_split=0.2, verbose=1, callbacks=[\n",
    "                        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "                        # Puedes agregar más callbacks como TensorBoard si es necesario\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28759957]\n",
      " [0.29684996]\n",
      " [0.08103671]\n",
      " [0.21346355]\n",
      " [0.07488883]\n",
      " [0.03030303]\n",
      " [0.12121203]\n",
      " [0.38580736]\n",
      " [0.23636364]\n",
      " [0.1928742 ]\n",
      " [0.85256139]\n",
      " [0.10277905]\n",
      " [0.25544017]\n",
      " [0.45757576]\n",
      " [0.12121212]\n",
      " [0.22629394]\n",
      " [0.27518364]\n",
      " [0.11818182]\n",
      " [0.22286139]\n",
      " [0.3968781 ]\n",
      " [0.21455645]\n",
      " [0.0969697 ]\n",
      " [0.40375974]\n",
      " [0.16670468]\n",
      " [0.14359714]\n",
      " [0.24327169]\n",
      " [0.17065472]\n",
      " [0.17154173]\n",
      " [0.27272727]\n",
      " [0.27081264]\n",
      " [0.03735455]\n",
      " [0.22537247]\n",
      " [0.22637714]\n",
      " [0.40341429]\n",
      " [0.26059091]\n",
      " [0.35898095]\n",
      " [0.24718563]\n",
      " [0.43204234]\n",
      " [0.24242424]\n",
      " [0.10821325]\n",
      " [0.27272727]\n",
      " [0.18787879]\n",
      " [0.26086848]\n",
      " [0.10493281]\n",
      " [0.28181818]\n",
      " [0.49090909]\n",
      " [0.14920294]\n",
      " [0.28787879]\n",
      " [0.5030303 ]\n",
      " [0.34135186]\n",
      " [0.12849212]\n",
      " [0.29964009]\n",
      " [0.29244745]\n",
      " [0.28484848]\n",
      " [0.29090909]\n",
      " [0.29596623]\n",
      " [0.08712017]\n",
      " [0.34545455]\n",
      " [0.67886381]\n",
      " [0.3030303 ]\n",
      " [0.78774701]\n",
      " [0.20167965]\n",
      " [0.22776797]\n",
      " [0.1030303 ]\n",
      " [0.38484848]\n",
      " [0.3701    ]\n",
      " [0.32575004]\n",
      " [0.21212121]\n",
      " [0.18411506]\n",
      " [0.47918442]\n",
      " [0.2274439 ]\n",
      " [0.18181818]\n",
      " [0.19646113]\n",
      " [0.14719134]\n",
      " [0.60480017]\n",
      " [0.1536458 ]\n",
      " [0.12716597]\n",
      " [0.16614771]\n",
      " [0.23534563]\n",
      " [0.2082342 ]\n",
      " [0.41311022]\n",
      " [0.09646104]\n",
      " [0.33296589]\n",
      " [0.23939394]\n",
      " [0.25153022]\n",
      " [0.21212121]\n",
      " [0.48484848]\n",
      " [0.35258831]\n",
      " [0.33333333]\n",
      " [0.22969697]\n",
      " [0.21212121]\n",
      " [0.10008035]\n",
      " [0.51114753]\n",
      " [0.1030303 ]\n",
      " [0.08381403]\n",
      " [0.65412736]\n",
      " [0.25509619]\n",
      " [0.1969697 ]\n",
      " [0.20607299]\n",
      " [0.22943074]\n",
      " [1.        ]\n",
      " [0.14545455]\n",
      " [0.15726805]\n",
      " [0.29907532]\n",
      " [0.3429974 ]\n",
      " [0.11699273]\n",
      " [0.20860052]\n",
      " [0.33769152]\n",
      " [0.17541117]\n",
      " [0.24396814]\n",
      " [0.11515152]\n",
      " [0.25831022]\n",
      " [0.2030303 ]\n",
      " [0.56666667]\n",
      " [0.07393939]\n",
      " [0.15688208]\n",
      " [0.64848485]\n",
      " [0.39393939]\n",
      " [0.26533931]\n",
      " [0.24545455]\n",
      " [0.40337723]\n",
      " [0.26318251]\n",
      " [0.22424242]\n",
      " [0.25041853]\n",
      " [0.20106459]\n",
      " [0.26542277]\n",
      " [0.37014312]\n",
      " [0.16500251]\n",
      " [0.32756442]\n",
      " [0.21170061]\n",
      " [0.17950537]\n",
      " [0.05491506]\n",
      " [0.30977758]\n",
      " [0.26820952]\n",
      " [0.24410052]\n",
      " [0.15493489]\n",
      " [0.18993723]\n",
      " [0.35475879]\n",
      " [0.51289385]\n",
      " [0.14927515]\n",
      " [0.16666667]\n",
      " [0.39931931]\n",
      " [0.42322338]\n",
      " [0.10114156]\n",
      " [0.10772944]\n",
      " [0.05781143]\n",
      " [0.33333333]\n",
      " [0.12702675]\n",
      " [0.14478216]\n",
      " [0.17881498]\n",
      " [0.23767714]\n",
      " [0.17907775]\n",
      " [0.24335186]\n",
      " [0.14592996]\n",
      " [0.090909  ]\n",
      " [0.15580268]\n",
      " [0.10506528]\n",
      " [0.16810312]\n",
      " [0.20821368]\n",
      " [0.23805654]\n",
      " [0.4065639 ]\n",
      " [0.30121212]\n",
      " [0.18157403]\n",
      " [0.12727273]\n",
      " [0.5030303 ]\n",
      " [0.41698874]\n",
      " [0.32676892]\n",
      " [0.18068701]\n",
      " [0.179451  ]\n",
      " [0.27441273]\n",
      " [0.38750156]\n",
      " [0.18071307]\n",
      " [0.24942095]\n",
      " [0.14566788]\n",
      " [0.1955187 ]\n",
      " [0.00143264]\n",
      " [0.21008234]\n",
      " [0.1969697 ]\n",
      " [0.18937203]\n",
      " [0.21212121]\n",
      " [0.15558632]\n",
      " [0.20102286]\n",
      " [0.33868615]\n",
      " [0.26458364]\n",
      " [0.42788797]\n",
      " [0.14484848]\n",
      " [0.12813316]\n",
      " [0.27278208]\n",
      " [0.18932424]\n",
      " [0.20248199]\n",
      " [0.13426476]\n",
      " [0.47600294]\n",
      " [0.19835714]\n",
      " [0.23295359]\n",
      " [0.3030303 ]\n",
      " [0.23636364]\n",
      " [0.1489374 ]\n",
      " [0.07694545]\n",
      " [0.13333333]\n",
      " [0.5644755 ]\n",
      " [0.45137117]\n",
      " [0.434991  ]\n",
      " [0.2004781 ]\n",
      " [0.29169359]\n",
      " [0.16604147]\n",
      " [0.48459671]\n",
      " [0.08140009]\n",
      " [0.19674494]\n",
      " [0.18921264]\n",
      " [0.27878788]\n",
      " [0.37528693]\n",
      " [0.3030303 ]\n",
      " [0.27192597]\n",
      " [0.36969697]\n",
      " [0.40909091]\n",
      " [0.35072407]\n",
      " [0.37363091]\n",
      " [0.35242857]\n",
      " [0.33840771]\n",
      " [0.23997524]\n",
      " [0.19787221]\n",
      " [0.27114009]\n",
      " [0.1650058 ]\n",
      " [0.03317039]\n",
      " [0.33030303]\n",
      " [0.10973203]\n",
      " [0.09943948]\n",
      " [0.16363636]\n",
      " [0.34148121]\n",
      " [0.31373082]\n",
      " [0.2709458 ]\n",
      " [0.33662952]\n",
      " [0.11692043]\n",
      " [0.09880225]\n",
      " [0.13030303]\n",
      " [0.18982329]\n",
      " [0.19261325]\n",
      " [0.36384537]\n",
      " [0.18224398]\n",
      " [0.18787879]\n",
      " [0.51464797]\n",
      " [0.3027574 ]\n",
      " [0.37305048]\n",
      " [0.39628762]\n",
      " [0.19410087]\n",
      " [0.35174693]\n",
      " [0.44545455]\n",
      " [0.11607602]\n",
      " [0.24801186]\n",
      " [0.21899333]\n",
      " [0.11922502]\n",
      " [0.35090909]\n",
      " [0.43948251]\n",
      " [0.23636364]\n",
      " [0.07878788]\n",
      " [0.10493394]\n",
      " [0.39681991]\n",
      " [0.17575758]\n",
      " [0.1718961 ]\n",
      " [0.14045948]\n",
      " [0.08913524]\n",
      " [0.30260632]\n",
      " [0.20176017]\n",
      " [0.22194736]\n",
      " [0.36363636]\n",
      " [0.07995974]\n",
      " [0.12229437]\n",
      " [0.08484848]\n",
      " [0.23399489]\n",
      " [0.16137143]\n",
      " [0.17099498]\n",
      " [0.41459671]\n",
      " [0.24205957]\n",
      " [0.22605532]\n",
      " [0.19858346]\n",
      " [0.14925974]\n",
      " [0.17257117]\n",
      " [0.14725455]\n",
      " [0.64010745]\n",
      " [0.1337271 ]\n",
      " [0.33201524]\n",
      " [0.21414078]\n",
      " [0.2266039 ]\n",
      " [0.21058848]\n",
      " [0.34358779]\n",
      " [0.14409662]\n",
      " [0.42242424]\n",
      " [0.25316571]\n",
      " [0.23854026]\n",
      " [0.29819827]\n",
      " [0.25836606]\n",
      " [0.23312563]\n",
      " [0.30406199]\n",
      " [0.35402745]\n",
      " [0.21429048]\n",
      " [0.26534805]\n",
      " [0.18311671]\n",
      " [0.        ]\n",
      " [0.20761662]\n",
      " [0.33787879]\n",
      " [0.15629004]\n",
      " [0.32775126]\n",
      " [0.14907784]\n",
      " [0.13333333]\n",
      " [0.0898968 ]\n",
      " [0.11184831]\n",
      " [0.32216329]\n",
      " [0.1941961 ]\n",
      " [0.1782729 ]\n",
      " [0.11752545]\n",
      " [0.30242424]\n",
      " [0.19883152]\n",
      " [0.36514589]\n",
      " [0.13987541]\n",
      " [0.26903896]\n",
      " [0.35932165]\n",
      " [0.29393939]\n",
      " [0.14242424]\n",
      " [0.18997861]\n",
      " [0.19422329]\n",
      " [0.19531576]\n",
      " [0.10452857]\n",
      " [0.34475186]\n",
      " [0.55757013]\n",
      " [0.11631195]\n",
      " [0.38429654]\n",
      " [0.33337082]\n",
      " [0.11499775]\n",
      " [0.32121212]\n",
      " [0.3887226 ]\n",
      " [0.32383108]\n",
      " [0.27721991]\n",
      " [0.19200926]\n",
      " [0.38649861]\n",
      " [0.33232312]\n",
      " [0.57575758]\n",
      " [0.47786918]\n",
      " [0.13939394]\n",
      " [0.07329177]\n",
      " [0.3030303 ]\n",
      " [0.69833247]\n",
      " [0.21212121]\n",
      " [0.19545455]\n",
      " [0.5077781 ]\n",
      " [0.16460762]\n",
      " [0.08344303]\n",
      " [0.04545455]\n",
      " [0.07571714]\n",
      " [0.38856113]\n",
      " [0.39965524]\n",
      " [0.34743593]\n",
      " [0.99999991]\n",
      " [0.10909091]\n",
      " [0.09794225]\n",
      " [0.14545455]\n",
      " [0.42727273]\n",
      " [0.33445887]\n",
      " [0.07272727]\n",
      " [0.14086978]\n",
      " [0.12568866]\n",
      " [0.4159219 ]\n",
      " [0.38184009]\n",
      " [0.26205991]\n",
      " [0.15648485]\n",
      " [0.1513258 ]\n",
      " [0.45231126]\n",
      " [0.26279714]\n",
      " [0.20311801]\n",
      " [0.13333333]\n",
      " [0.11018078]\n",
      " [0.26547714]\n",
      " [0.18795766]\n",
      " [0.2789942 ]\n",
      " [0.11191584]\n",
      " [0.17009152]\n",
      " [0.29839558]\n",
      " [0.19545455]\n",
      " [0.37873117]\n",
      " [0.26053013]\n",
      " [0.38796511]\n",
      " [0.13025957]\n",
      " [0.4575561 ]\n",
      " [0.18612182]\n",
      " [0.50263532]\n",
      " [0.324009  ]\n",
      " [0.17002242]\n",
      " [0.24545455]\n",
      " [0.30202173]\n",
      " [0.12121212]\n",
      " [0.26695853]\n",
      " [0.21375835]\n",
      " [0.4899871 ]\n",
      " [0.15789333]\n",
      " [0.26699359]\n",
      " [0.428709  ]\n",
      " [0.31844797]\n",
      " [0.31352866]\n",
      " [0.18979264]\n",
      " [0.17527541]\n",
      " [0.26395844]\n",
      " [0.48481593]\n",
      " [0.27104693]\n",
      " [0.35218052]\n",
      " [0.1700613 ]\n",
      " [0.36619775]\n",
      " [0.25135628]\n",
      " [0.43720606]\n",
      " [0.04545455]\n",
      " [0.49914606]\n",
      " [0.12576519]\n",
      " [0.17875065]\n",
      " [0.0969697 ]\n",
      " [0.15183203]\n",
      " [0.28730641]\n",
      " [0.37727273]\n",
      " [0.24915048]\n",
      " [0.29969671]\n",
      " [0.30964459]\n",
      " [0.31686502]\n",
      " [0.2       ]\n",
      " [0.29489013]\n",
      " [0.32341584]\n",
      " [0.13366294]\n",
      " [0.24926935]\n",
      " [0.09802355]\n",
      " [0.15151515]\n",
      " [0.03719593]\n",
      " [0.15431446]\n",
      " [0.11851541]\n",
      " [0.03333333]\n",
      " [0.25377576]\n",
      " [0.60308294]\n",
      " [0.06666667]\n",
      " [0.45038857]\n",
      " [0.10472113]\n",
      " [0.34555255]\n",
      " [0.82508727]\n",
      " [0.13415922]\n",
      " [0.25530485]\n",
      " [0.11999264]\n",
      " [0.07030848]\n",
      " [0.26826208]\n",
      " [0.43636364]\n",
      " [0.29076494]\n",
      " [0.11666805]\n",
      " [0.22412355]\n",
      " [0.31889723]\n",
      " [0.48424242]\n",
      " [0.58527732]\n",
      " [0.12940147]\n",
      " [0.32500528]\n",
      " [0.18431212]\n",
      " [0.14700918]\n",
      " [0.42430199]\n",
      " [0.1400226 ]\n",
      " [0.20606061]\n",
      " [0.10588554]\n",
      " [0.24256693]\n",
      " [0.12734026]\n",
      " [0.12793827]\n",
      " [0.80862537]\n",
      " [0.15151515]\n",
      " [0.40878519]\n",
      " [0.15067835]\n",
      " [0.07910364]\n",
      " [0.24089195]\n",
      " [0.0789568 ]\n",
      " [0.19775212]\n",
      " [0.65695853]\n",
      " [0.14258043]\n",
      " [0.24134442]\n",
      " [0.27226156]\n",
      " [0.16630078]\n",
      " [0.35392346]\n",
      " [0.21094139]\n",
      " [0.23999117]\n",
      " [0.25561593]\n",
      " [0.14210043]\n",
      " [0.13333333]\n",
      " [0.31126468]\n",
      " [0.36363636]\n",
      " [0.19160987]\n",
      " [0.17878035]\n",
      " [0.14299264]\n",
      " [0.30647593]\n",
      " [0.27272727]\n",
      " [0.2030303 ]\n",
      " [0.27667013]\n",
      " [0.24848485]\n",
      " [0.25699316]\n",
      " [0.12267212]\n",
      " [0.21676173]\n",
      " [0.17275255]\n",
      " [0.37575758]\n",
      " [0.32487212]\n",
      " [0.32167481]\n",
      " [0.14545455]\n",
      " [0.26666667]\n",
      " [0.21229792]\n",
      " [0.0886974 ]\n",
      " [0.32176753]\n",
      " [0.18855706]\n",
      " [0.16272528]\n",
      " [0.10909091]\n",
      " [0.20195749]\n",
      " [0.41879506]\n",
      " [0.34395905]\n",
      " [0.10639766]\n",
      " [0.27272727]\n",
      " [0.19976286]\n",
      " [0.45556978]\n",
      " [0.15151515]\n",
      " [0.42449126]\n",
      " [0.29964242]\n",
      " [0.10936216]\n",
      " [0.25693835]\n",
      " [0.36893662]\n",
      " [0.34848485]\n",
      " [0.43141749]\n",
      " [0.08329576]\n",
      " [0.13890355]\n",
      " [0.53337645]\n",
      " [0.16666667]\n",
      " [0.4971097 ]\n",
      " [0.57575758]\n",
      " [0.46537957]\n",
      " [0.5969697 ]\n",
      " [0.32866563]\n",
      " [0.30818571]\n",
      " [0.08945558]\n",
      " [0.13939394]\n",
      " [0.47019463]\n",
      " [0.11575758]\n",
      " [0.51515152]\n",
      " [0.40909091]\n",
      " [0.24048563]\n",
      " [0.22890113]\n",
      " [0.21818182]\n",
      " [0.16456104]\n",
      " [0.2595155 ]\n",
      " [0.1622981 ]\n",
      " [0.39262727]\n",
      " [0.15151515]\n",
      " [0.03030303]\n",
      " [0.23364225]\n",
      " [0.08982736]\n",
      " [0.35049576]\n",
      " [0.45719455]\n",
      " [0.41868468]\n",
      " [0.43604848]\n",
      " [0.18369255]\n",
      " [0.1554058 ]\n",
      " [0.2989387 ]\n",
      " [0.68024494]\n",
      " [0.17393983]\n",
      " [0.32133784]\n",
      " [0.22111879]\n",
      " [0.25656277]\n",
      " [0.26121212]\n",
      " [0.        ]\n",
      " [0.22265169]\n",
      " [0.17398797]\n",
      " [0.39810597]\n",
      " [0.17193905]\n",
      " [0.26349229]\n",
      " [0.18170026]\n",
      " [0.24235411]\n",
      " [0.24193827]\n",
      " [0.18042242]\n",
      " [0.68488779]\n",
      " [0.22424242]\n",
      " [0.0569697 ]\n",
      " [0.11035429]\n",
      " [0.16081082]\n",
      " [0.28913134]\n",
      " [0.35859723]\n",
      " [0.17575758]\n",
      " [0.17002909]\n",
      " [0.23506944]\n",
      " [0.22631082]\n",
      " [0.24550433]\n",
      " [0.91456   ]\n",
      " [0.55504823]\n",
      " [0.23395835]\n",
      " [0.23232494]\n",
      " [0.17774848]\n",
      " [0.3030303 ]\n",
      " [0.21071801]\n",
      " [0.28227818]\n",
      " [0.14769602]\n",
      " [0.16328745]\n",
      " [0.10008658]\n",
      " [0.42273221]\n",
      " [0.16495455]\n",
      " [0.24842017]\n",
      " [0.16666667]\n",
      " [0.14988156]\n",
      " [0.15726113]\n",
      " [0.35474009]\n",
      " [0.07028623]\n",
      " [0.17575758]\n",
      " [0.32057861]\n",
      " [0.23479991]\n",
      " [0.16070312]\n",
      " [0.20243325]\n",
      " [0.20680935]\n",
      " [0.21297385]\n",
      " [0.07878788]\n",
      " [0.24162199]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      "Métricas de Entrenamiento (desnormalizadas):\n",
      "MAE: 1252974.6806\n",
      "RMSE: 1662809.6442\n",
      "R²: -0.0004\n",
      "El RMSE es superior al 15% de la media: 35.24%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "y_pred_unscaled = price_scaler.inverse_transform(y_pred)\n",
    "y_test_unscaled = price_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Calcular métricas sobre los valores desnormalizados\n",
    "mae_train_real = mean_absolute_error(y_pred_unscaled, y_test_unscaled)\n",
    "\n",
    "rmse_train_real = np.sqrt(mean_squared_error(y_pred_unscaled, y_test_unscaled))\n",
    "\n",
    "r2_train_real = r2_score(y_test_unscaled, y_pred_unscaled)\n",
    "\n",
    "# Calcular el RMSE como porcentaje de la media de los datos de prueba desnormalizados\n",
    "mean_value_real = np.mean(y_test_unscaled)\n",
    "rmse_percentage_real = (rmse_train_real / mean_value_real) * 100\n",
    "\n",
    "# Imprimir las métricas desnormalizadas\n",
    "print(\"\\nMétricas de Entrenamiento (desnormalizadas):\")\n",
    "print(f'MAE: {mae_train_real:.4f}')\n",
    "print(f'RMSE: {rmse_train_real:.4f}')\n",
    "print(f'R²: {r2_train_real:.4f}')\n",
    "\n",
    "if rmse_percentage_real > 15:\n",
    "    print(f'El RMSE es superior al 15% de la media: {rmse_percentage_real:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m plt.figure(figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      3\u001b[39m plt.subplot(\u001b[32m121\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \u001b[33m'\u001b[39m\u001b[33mr--\u001b[39m\u001b[33m'\u001b[39m, lw=\u001b[32m2\u001b[39m)\n\u001b[32m      6\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mValores reales\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/codes/projects/PropNet/PropNet-project/environment/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/codes/projects/PropNet/PropNet-project/environment/lib/python3.11/site-packages/matplotlib/pyplot.py:3937\u001b[39m, in \u001b[36mscatter\u001b[39m\u001b[34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, data, **kwargs)\u001b[39m\n\u001b[32m   3917\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.scatter)\n\u001b[32m   3918\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscatter\u001b[39m(\n\u001b[32m   3919\u001b[39m     x: \u001b[38;5;28mfloat\u001b[39m | ArrayLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3935\u001b[39m     **kwargs,\n\u001b[32m   3936\u001b[39m ) -> PathCollection:\n\u001b[32m-> \u001b[39m\u001b[32m3937\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3939\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3940\u001b[39m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3947\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3949\u001b[39m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[43m=\u001b[49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3952\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3954\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3955\u001b[39m     sci(__ret)\n\u001b[32m   3956\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/codes/projects/PropNet/PropNet-project/environment/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/codes/projects/PropNet/PropNet-project/environment/lib/python3.11/site-packages/matplotlib/__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/codes/projects/PropNet/PropNet-project/environment/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4930\u001b[39m, in \u001b[36mAxes.scatter\u001b[39m\u001b[34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, **kwargs)\u001b[39m\n\u001b[32m   4928\u001b[39m y = np.ma.ravel(y)\n\u001b[32m   4929\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.size != y.size:\n\u001b[32m-> \u001b[39m\u001b[32m4930\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mx and y must be the same size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4932\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4933\u001b[39m     s = (\u001b[32m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33m_internal.classic_mode\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m   4934\u001b[39m          mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33mlines.markersize\u001b[39m\u001b[33m'\u001b[39m] ** \u001b[32m2.0\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(y_train, y_pred, alpha=0.5)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "plt.xlabel('Valores reales')\n",
    "plt.ylabel('Predicciones')\n",
    "plt.title(f'Entrenamiento\\nMAE: {mae_train_real:.4f}, RMSE: {rmse_train_real:.4f}')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Valores reales')\n",
    "plt.ylabel('Predicciones')\n",
    "plt.title(f'Prueba\\nMAE: {mae_train_real:.4f}, RMSE: {rmse_train_real:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
