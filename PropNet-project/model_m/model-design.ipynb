{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design: Neutal Network creation with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LIBS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Lion\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ocultar advertencias de TensorFlow y otros warnings innecesarios\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Oculta logs de TensorFlow (0 = todos, 1 = INFO, 2 = WARNING, 3 = ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Oculta warnings de Python\n",
    "\n",
    "# Desactivar ejecución ansiosa (si no la necesitas)\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "PRICE = 'SalePrice'\n",
    "# PRICE = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records (rows): 1460\n",
      "Number of columns: 231\n",
      "\n",
      "Column names:\n",
      "['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'ExterQual_TA', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'KitchenQual_TA', 'YearRemodAdd', 'Foundation_PConc', 'MasVnrArea', 'Fireplaces', 'ExterQual_Gd', 'BsmtQual_TA', 'BsmtFinType1_GLQ', 'Neighborhood_NridgHt', 'BsmtFinSF1', 'SaleType_New', 'SaleCondition_Partial', 'Foundation_CBlock', 'Neighborhood_NoRidge', 'WoodDeckSF', 'KitchenQual_Gd', '2ndFlrSF', 'OpenPorchSF', 'HeatingQC_TA', 'BsmtExposure_Gd', 'Exterior2nd_VinylSd', 'Exterior1st_VinylSd', 'MSZoning_RM', 'HalfBath', 'LotShape_Reg', 'LotArea', 'BsmtExposure_No', 'CentralAir', 'MSZoning_RL', 'HouseStyle_2Story', 'SaleType_WD', 'Electrical_SBrkr', 'RoofStyle_Hip', 'BsmtQual_Gd', 'BsmtFullBath', 'RoofStyle_Gable', 'Neighborhood_StoneBr', 'BsmtUnfSF', 'PavedDrive', 'Neighborhood_OldTown', 'Neighborhood_NAmes', 'Neighborhood_Edwards', 'RoofMatl_WdShngl', 'BedroomAbvGr', 'Exterior1st_MetalSd', 'Neighborhood_IDOTRR', 'Exterior2nd_MetalSd', 'Exterior2nd_Wd Sdng', 'Exterior1st_Wd Sdng', 'KitchenQual_Fa', 'SaleCondition_Normal', 'Neighborhood_BrkSide', 'LotConfig_CulDSac', 'Neighborhood_Somerst', 'ExterCond_Fa', 'KitchenAbvGr', 'BsmtFinType1_Rec', 'HeatingQC_Gd', 'HeatingQC_Fa', 'Exterior1st_CemntBd', 'BsmtFinType1_BLQ', 'BsmtQual_Fa', 'EnclosedPorch', 'Neighborhood_Sawyer', 'Exterior2nd_CmentBd', 'Electrical_FuseF', 'Neighborhood_Timber', 'LotShape_IR2', 'LandContour_HLS', 'Foundation_Slab', 'BsmtFinType2_Unf', 'Condition1_Feedr', 'Functional_Typ', 'ExterQual_Fa', 'BldgType_Duplex', 'Condition1_Norm', 'Neighborhood_MeadowV', 'ScreenPorch', 'ExterCond_TA', 'RoofMatl_CompShg', 'Neighborhood_BrDale', 'BsmtCond_TA', 'BldgType_Twnhs', 'BldgType_2fmCon', 'Exterior1st_HdBoard', 'HouseStyle_SFoyer', 'Heating_GasA', 'PoolArea', 'Heating_Grav', 'MSZoning_FV', 'BsmtCond_Gd', 'HouseStyle_1.5Unf', 'BsmtFinType1_LwQ', 'MSSubClass', 'BsmtFinType1_Unf', 'LotConfig_Inside', 'OverallCond', 'Exterior2nd_ImStucc', 'Neighborhood_CollgCr', 'Functional_Min2', 'Neighborhood_Crawfor', 'Functional_Maj2', 'Exterior2nd_HdBoard', 'MSZoning_RH', 'Functional_Min1', 'Neighborhood_SWISU', 'Neighborhood_Veenker', 'HouseStyle_1Story', 'Heating_Wall', 'Neighborhood_Mitchel', 'BsmtFinType2_BLQ', 'Neighborhood_ClearCr', 'BsmtCond_Po', 'Exterior2nd_Plywood', 'Exterior1st_WdShing', 'Exterior1st_BrkComm', 'SaleCondition_AdjLand', 'ExterCond_Gd', 'Condition1_PosN', 'Condition2_PosN', 'Condition2_Feedr', 'Electrical_FuseP', 'Condition2_PosA', 'Exterior2nd_Brk Cmn', 'Condition1_RRAe', 'SaleCondition_Family', 'MoSold', 'LandContour_Low', 'Exterior2nd_Other', 'RoofMatl_WdShake', '3SsnPorch', 'BsmtExposure_Mn', 'LandSlope_Mod', 'Exterior2nd_Stucco', 'Condition1_PosA', 'SaleType_ConLD', 'SaleType_Con', 'Street_Pave', 'Exterior2nd_Wd Shng', 'BsmtFinType2_Rec', 'Condition2_RRNn', 'HouseStyle_SLvl', 'Neighborhood_NPkVill', 'BsmtFinType2_LwQ', 'Electrical_Mix', 'LotShape_IR3', 'HouseStyle_2.5Fin', 'Exterior1st_Stone', 'Neighborhood_Gilbert', 'RoofStyle_Gambrel', 'SaleType_Oth', 'ExterCond_Po', 'Exterior1st_BrkFace', 'HeatingQC_Po', 'Condition2_Norm', 'Exterior1st_Stucco', 'YrSold', 'LandSlope_Sev', 'LandContour_Lvl', 'SaleType_ConLw', 'Exterior1st_ImStucc', 'Exterior1st_AsphShn', 'HouseStyle_2.5Unf', 'Heating_OthW', 'LowQualFinSF', 'Exterior1st_CBlock', 'Exterior2nd_CBlock', 'Exterior2nd_BrkFace', 'Exterior2nd_AsphShn', 'Neighborhood_NWAmes', 'Condition1_RRNn', 'Id', 'MiscVal', 'RoofStyle_Shed', 'Neighborhood_Blueste', 'Heating_GasW', 'RoofMatl_Membran', 'SaleType_CWD', 'LotConfig_FR3', 'Exterior1st_Plywood', 'KPI_mult_Id_ExterQual_Gd', 'KPI_mult_Id_BsmtFinType1_GLQ', 'KPI_mult_Id_BsmtQual_TA', 'KPI_mult_Id_Neighborhood_NridgHt', 'KPI_mult_Id_Neighborhood_NoRidge', 'KPI_mult_Id_SaleType_New', 'KPI_mult_Id_SaleCondition_Partial', 'KPI_mult_Id_BsmtFinSF1', 'KPI_mult_Id_Foundation_CBlock', 'KPI_mult_Id_BsmtExposure_Gd', 'KPI_mult_Id_HeatingQC_TA', 'KPI_mult_Id_WoodDeckSF', 'KPI_mult_Id_OpenPorchSF', 'KPI_mult_Id_KitchenQual_Gd', 'KPI_mult_Id_2ndFlrSF', 'KPI_mult_Id_MSZoning_RM', 'KPI_mult_Id_Exterior2nd_VinylSd', 'KPI_mult_Id_Exterior1st_VinylSd', 'KPI_mult_Id_LotShape_Reg', 'KPI_mult_Id_LotArea', 'KPI_div_Id_LotArea', 'KPI_mult_Id_HalfBath', 'KPI_mult_Id_RoofStyle_Hip', 'KPI_mult_Id_BsmtExposure_No', 'KPI_mult_Id_BsmtQual_Gd', 'KPI_mult_Id_HouseStyle_2Story', 'KPI_div_Id_Neighborhood_StoneBr', 'KPI_mult_Id_RoofStyle_Gable', 'KPI_mult_Id_Neighborhood_StoneBr', 'KPI_mult_Id_BsmtFullBath', 'KPI_mult_Id_Neighborhood_OldTown', 'KPI_mult_Id_Neighborhood_NAmes', 'KPI_div_Id_Neighborhood_NridgHt', 'KPI_mult_Id_Neighborhood_Edwards', 'KPI_mult_Id_BsmtUnfSF', 'KPI_mult_Id_Exterior1st_MetalSd', 'KPI_mult_Id_SaleType_WD', 'KPI_div_Id_SaleType_New', 'KPI_mult_Id_Exterior2nd_Wd Sdng', 'KPI_div_Id_SaleCondition_Partial']\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE DATASET FROM PREVIOUS CSV\n",
    "# path = '/home/mike/Escritorio/codes/projects/POLARAI/neural-nets/data_m/csv_files/processed_data.csv'\n",
    "path = '/home/mike/Escritorio/codes/projects/POLARAI/neural-nets/data_m/csv_files/03_kpi_data.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Get number of rows and columns\n",
    "num_rows, num_cols = data.shape\n",
    "\n",
    "print(f\"Number of records (rows): {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\")\n",
    "\n",
    "# Optional: Display column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW DATA GENERATION\n",
    "to get more than the 1500 columns we have, we are going to generate new data using the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[PRICE])\n",
    "y = data[PRICE] \n",
    "\n",
    "# First check for and handle infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Then handle NaN values\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# ==================== DATA AUGMENTATION ====================\n",
    "def augment_data(X, y, num_samples=1000):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Interpolación entre puntos reales\n",
    "    idx1 = np.random.randint(0, len(X), num_samples)\n",
    "    idx2 = np.random.randint(0, len(X), num_samples)\n",
    "    \n",
    "    alpha = np.random.uniform(0, 1, num_samples).reshape(-1, 1)\n",
    "    X_aug = alpha * X.iloc[idx1].values + (1 - alpha) * X.iloc[idx2].values\n",
    "    y_aug = alpha.flatten() * y.iloc[idx1].values + (1 - alpha.flatten()) * y.iloc[idx2].values\n",
    "    \n",
    "    # Agregar ruido gaussiano\n",
    "    noise_X = np.random.normal(0, 0.01, X_aug.shape)\n",
    "    noise_y = np.random.normal(0, 0.01, y_aug.shape)\n",
    "    \n",
    "    X_aug += noise_X\n",
    "    y_aug += noise_y\n",
    "    \n",
    "    X_aug = pd.DataFrame(X_aug, columns=X.columns)\n",
    "    y_aug = pd.Series(y_aug, name=y.name)\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Aplicar aumentación\n",
    "X_aug, y_aug = augment_data(X, y, num_samples=11000)\n",
    "X = pd.concat([X, X_aug], axis=0)\n",
    "y = pd.concat([y, y_aug], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE THE DATA (MIN-MAX 1,0)\n",
    "price_scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "all_scaled_price = price_scaler.fit_transform(y.to_numpy().reshape(-1, 1)) \n",
    "all_scaled_vars = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_scaled_vars, all_scaled_price, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9968, 230)\n",
      "(9968, 1)\n",
      "(2492, 230)\n",
      "(2492, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "y_train_scaled = y_train\n",
    "y_test_scaled = y_test\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL DEFINITION FUNCTION\n",
    "\n",
    "def build_model(optimizer_name='SGD'):\n",
    "    optimizers_dict = {\n",
    "        'Adam': Adam(learning_rate=0.001),\n",
    "        'SGD': SGD(learning_rate=0.001, momentum=0.9),\n",
    "        'RMSprop': RMSprop(learning_rate=0.001),\n",
    "        'Adagrad': Adagrad(learning_rate=0.001),\n",
    "        'Adadelta': Adadelta(learning_rate=1.0),\n",
    "        'Adamax': Adamax(learning_rate=0.002),\n",
    "        'Nadam': Nadam(learning_rate=0.001),\n",
    "        'Lion' : Lion(learning_rate=0.001)\n",
    "    }\n",
    "\n",
    "    if optimizer_name not in optimizers_dict:\n",
    "        raise ValueError(f\"Optimizador {optimizer_name} no reconocido. Opciones válidas: {list(optimizers_dict.keys())}\")\n",
    "\n",
    "    optimizer = optimizers_dict[optimizer_name]\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout aumentado\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout aumentado\n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout en cada capa\n",
    "        layers.Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout en cada capa\n",
    "        layers.Dense(1, activation='linear')  # Capa de salida\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-FOLDING TO SELECT BEST OPTIMIZER\n",
    "\n",
    "# # Definir validación cruzada\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Lista de optimizadores a probar\n",
    "# optimizers = ['Adam', 'SGD', 'Adadelta', 'Lion']\n",
    "\n",
    "# # Diccionario para almacenar resultados\n",
    "# results = {}\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "# for opt_name in optimizers:\n",
    "#     fold_mae = []\n",
    "\n",
    "#     print(f\"Entrenando con el optimizador: {opt_name}\")\n",
    "\n",
    "#     for train_idx, val_idx in kfold.split(X_train_scaled):\n",
    "#         X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "#         y_train_fold, y_val_fold = y_train_scaled[train_idx], y_train_scaled[val_idx]\n",
    "\n",
    "#         try:\n",
    "#             # Crear un nuevo modelo con el optimizador especificado\n",
    "#             model = build_model(optimizer_name=opt_name)\n",
    "\n",
    "#             # Entrenar el modelo\n",
    "#             history = model.fit(X_train_fold, y_train_fold, epochs=30, batch_size=24,\n",
    "#                                         validation_data=(X_val_fold, y_val_fold), verbose=0,\n",
    "#                                         callbacks=[early_stopping])\n",
    "\n",
    "#             # Evaluar el modelo\n",
    "#             val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)[1]\n",
    "#             fold_mae.append(val_mae)\n",
    "\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Error con optimizador {opt_name}: {e}\")\n",
    "#             continue  # Saltar a la siguiente iteración si hay un error\n",
    "\n",
    "#     # Guardar el MAE promedio en los resultados solo si hubo evaluaciones válidas\n",
    "#     if fold_mae:\n",
    "#         results[opt_name] = np.mean(fold_mae)\n",
    "#     else:\n",
    "#         results[opt_name] = float('inf')  # Asignar un valor alto si no se pudo entrenar\n",
    "\n",
    "# print('============================================================')\n",
    "# # Imprimir los resultados de validación cruzada\n",
    "# print(\"\\nResultados de Validación Cruzada (MAE promedio):\")\n",
    "# for opt, mae in results.items():\n",
    "#     print(f\"{opt}: {mae:.4f}\")\n",
    "# print('============================================================')\n",
    "# # Seleccionar el mejor optimizador\n",
    "# best_optimizer_name = min(results, key=results.get)\n",
    "# print(f\"\\nMejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "# print('============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor optimizador seleccionado: Adam\n"
     ]
    }
   ],
   "source": [
    "# CREATE THE MODEL AND\n",
    "# best_optimizer_name = min(results, key=results.get)\n",
    "best_optimizer_name = 'Adam'\n",
    "print(f\"Mejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "\n",
    "model = build_model(optimizer_name=best_optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - loss: 23.7614 - mae: 0.6189 - val_loss: 4.3241 - val_mae: 0.2966 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9738 - mae: 0.3363 - val_loss: 1.0599 - val_mae: 0.2665 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0311 - mae: 0.3331 - val_loss: 0.7191 - val_mae: 0.2807 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7613 - mae: 0.3275 - val_loss: 0.6117 - val_mae: 0.2621 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6911 - mae: 0.3243 - val_loss: 0.5621 - val_mae: 0.2573 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6143 - mae: 0.3129 - val_loss: 0.5322 - val_mae: 0.2565 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6233 - mae: 0.3278 - val_loss: 0.5044 - val_mae: 0.2476 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6142 - mae: 0.3270 - val_loss: 0.5061 - val_mae: 0.2695 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5708 - mae: 0.3168 - val_loss: 0.4863 - val_mae: 0.2631 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5602 - mae: 0.3253 - val_loss: 0.4720 - val_mae: 0.2644 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5522 - mae: 0.3242 - val_loss: 0.4540 - val_mae: 0.2534 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5779 - mae: 0.3356 - val_loss: 0.4428 - val_mae: 0.2532 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5213 - mae: 0.3230 - val_loss: 0.4318 - val_mae: 0.2497 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5207 - mae: 0.3232 - val_loss: 0.4308 - val_mae: 0.2532 - learning_rate: 0.0010\n",
      "Epoch 15/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5189 - mae: 0.3185 - val_loss: 0.4232 - val_mae: 0.2505 - learning_rate: 0.0010\n",
      "Epoch 16/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5036 - mae: 0.3172 - val_loss: 0.4281 - val_mae: 0.2546 - learning_rate: 0.0010\n",
      "Epoch 17/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5071 - mae: 0.3149 - val_loss: 0.4089 - val_mae: 0.2373 - learning_rate: 0.0010\n",
      "Epoch 18/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5093 - mae: 0.3194 - val_loss: 0.4165 - val_mae: 0.2444 - learning_rate: 0.0010\n",
      "Epoch 19/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4838 - mae: 0.3125 - val_loss: 0.4126 - val_mae: 0.2460 - learning_rate: 0.0010\n",
      "Epoch 20/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5100 - mae: 0.3210 - val_loss: 0.4023 - val_mae: 0.2383 - learning_rate: 0.0010\n",
      "Epoch 21/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4902 - mae: 0.3146 - val_loss: 0.4038 - val_mae: 0.2402 - learning_rate: 0.0010\n",
      "Epoch 22/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4815 - mae: 0.3122 - val_loss: 0.4068 - val_mae: 0.2421 - learning_rate: 0.0010\n",
      "Epoch 23/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5113 - mae: 0.3220 - val_loss: 0.4053 - val_mae: 0.2398 - learning_rate: 0.0010\n",
      "Epoch 24/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4744 - mae: 0.3061 - val_loss: 0.3960 - val_mae: 0.2374 - learning_rate: 0.0010\n",
      "Epoch 25/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4784 - mae: 0.3125 - val_loss: 0.4038 - val_mae: 0.2463 - learning_rate: 0.0010\n",
      "Epoch 26/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4885 - mae: 0.3107 - val_loss: 0.3890 - val_mae: 0.2317 - learning_rate: 0.0010\n",
      "Epoch 27/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4748 - mae: 0.3108 - val_loss: 0.4005 - val_mae: 0.2422 - learning_rate: 0.0010\n",
      "Epoch 28/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4736 - mae: 0.3072 - val_loss: 0.4091 - val_mae: 0.2548 - learning_rate: 0.0010\n",
      "Epoch 29/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4915 - mae: 0.3100 - val_loss: 0.3947 - val_mae: 0.2381 - learning_rate: 0.0010\n",
      "Epoch 30/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4629 - mae: 0.3050 - val_loss: 0.3934 - val_mae: 0.2390 - learning_rate: 0.0010\n",
      "Epoch 31/250\n",
      "\u001b[1m 90/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4760 - mae: 0.3166\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4765 - mae: 0.3154 - val_loss: 0.4076 - val_mae: 0.2503 - learning_rate: 0.0010\n",
      "Epoch 32/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4778 - mae: 0.3086 - val_loss: 0.3608 - val_mae: 0.2309 - learning_rate: 5.0000e-04\n",
      "Epoch 33/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4618 - mae: 0.3124 - val_loss: 0.3608 - val_mae: 0.2340 - learning_rate: 5.0000e-04\n",
      "Epoch 34/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4684 - mae: 0.3115 - val_loss: 0.3638 - val_mae: 0.2356 - learning_rate: 5.0000e-04\n",
      "Epoch 35/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4656 - mae: 0.3132 - val_loss: 0.3592 - val_mae: 0.2342 - learning_rate: 5.0000e-04\n",
      "Epoch 36/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4559 - mae: 0.3095 - val_loss: 0.3621 - val_mae: 0.2361 - learning_rate: 5.0000e-04\n",
      "Epoch 37/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4456 - mae: 0.3053 - val_loss: 0.3627 - val_mae: 0.2358 - learning_rate: 5.0000e-04\n",
      "Epoch 38/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4409 - mae: 0.3026 - val_loss: 0.3561 - val_mae: 0.2299 - learning_rate: 5.0000e-04\n",
      "Epoch 39/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4245 - mae: 0.2967 - val_loss: 0.3507 - val_mae: 0.2243 - learning_rate: 5.0000e-04\n",
      "Epoch 40/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4594 - mae: 0.3082 - val_loss: 0.3547 - val_mae: 0.2294 - learning_rate: 5.0000e-04\n",
      "Epoch 41/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4565 - mae: 0.3081 - val_loss: 0.3543 - val_mae: 0.2290 - learning_rate: 5.0000e-04\n",
      "Epoch 42/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4436 - mae: 0.3015 - val_loss: 0.3513 - val_mae: 0.2277 - learning_rate: 5.0000e-04\n",
      "Epoch 43/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4463 - mae: 0.3073 - val_loss: 0.3634 - val_mae: 0.2410 - learning_rate: 5.0000e-04\n",
      "Epoch 44/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4333 - mae: 0.2992 - val_loss: 0.3490 - val_mae: 0.2261 - learning_rate: 5.0000e-04\n",
      "Epoch 45/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4604 - mae: 0.3076 - val_loss: 0.3482 - val_mae: 0.2249 - learning_rate: 5.0000e-04\n",
      "Epoch 46/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4368 - mae: 0.3005 - val_loss: 0.3491 - val_mae: 0.2263 - learning_rate: 5.0000e-04\n",
      "Epoch 47/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4569 - mae: 0.3080 - val_loss: 0.3659 - val_mae: 0.2413 - learning_rate: 5.0000e-04\n",
      "Epoch 48/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4396 - mae: 0.3052 - val_loss: 0.3499 - val_mae: 0.2275 - learning_rate: 5.0000e-04\n",
      "Epoch 49/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4343 - mae: 0.2975 - val_loss: 0.3482 - val_mae: 0.2221 - learning_rate: 5.0000e-04\n",
      "Epoch 50/250\n",
      "\u001b[1m 91/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4191 - mae: 0.2969\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4226 - mae: 0.2977 - val_loss: 0.3604 - val_mae: 0.2364 - learning_rate: 5.0000e-04\n",
      "Epoch 51/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4128 - mae: 0.2934 - val_loss: 0.3385 - val_mae: 0.2294 - learning_rate: 2.5000e-04\n",
      "Epoch 52/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4236 - mae: 0.3027 - val_loss: 0.3303 - val_mae: 0.2214 - learning_rate: 2.5000e-04\n",
      "Epoch 53/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4163 - mae: 0.3020 - val_loss: 0.3345 - val_mae: 0.2279 - learning_rate: 2.5000e-04\n",
      "Epoch 54/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4207 - mae: 0.3013 - val_loss: 0.3357 - val_mae: 0.2285 - learning_rate: 2.5000e-04\n",
      "Epoch 55/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4197 - mae: 0.2988 - val_loss: 0.3344 - val_mae: 0.2253 - learning_rate: 2.5000e-04\n",
      "Epoch 56/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4312 - mae: 0.3055 - val_loss: 0.3355 - val_mae: 0.2272 - learning_rate: 2.5000e-04\n",
      "Epoch 57/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4368 - mae: 0.3060 - val_loss: 0.3292 - val_mae: 0.2193 - learning_rate: 2.5000e-04\n",
      "Epoch 58/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4137 - mae: 0.2956 - val_loss: 0.3293 - val_mae: 0.2209 - learning_rate: 2.5000e-04\n",
      "Epoch 59/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4210 - mae: 0.3005 - val_loss: 0.3358 - val_mae: 0.2302 - learning_rate: 2.5000e-04\n",
      "Epoch 60/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4012 - mae: 0.2924 - val_loss: 0.3313 - val_mae: 0.2240 - learning_rate: 2.5000e-04\n",
      "Epoch 61/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4112 - mae: 0.2960 - val_loss: 0.3345 - val_mae: 0.2274 - learning_rate: 2.5000e-04\n",
      "Epoch 62/250\n",
      "\u001b[1m 90/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4206 - mae: 0.3021\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4195 - mae: 0.3003 - val_loss: 0.3377 - val_mae: 0.2297 - learning_rate: 2.5000e-04\n",
      "Epoch 63/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3963 - mae: 0.2907 - val_loss: 0.3280 - val_mae: 0.2278 - learning_rate: 1.2500e-04\n",
      "Epoch 64/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4211 - mae: 0.3031 - val_loss: 0.3244 - val_mae: 0.2248 - learning_rate: 1.2500e-04\n",
      "Epoch 65/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3964 - mae: 0.2930 - val_loss: 0.3245 - val_mae: 0.2250 - learning_rate: 1.2500e-04\n",
      "Epoch 66/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3946 - mae: 0.2884 - val_loss: 0.3240 - val_mae: 0.2245 - learning_rate: 1.2500e-04\n",
      "Epoch 67/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4014 - mae: 0.2946 - val_loss: 0.3243 - val_mae: 0.2240 - learning_rate: 1.2500e-04\n",
      "Epoch 68/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3944 - mae: 0.2951 - val_loss: 0.3241 - val_mae: 0.2238 - learning_rate: 1.2500e-04\n",
      "Epoch 69/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4278 - mae: 0.2984 - val_loss: 0.3225 - val_mae: 0.2229 - learning_rate: 1.2500e-04\n",
      "Epoch 70/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4012 - mae: 0.2958 - val_loss: 0.3240 - val_mae: 0.2254 - learning_rate: 1.2500e-04\n",
      "Epoch 71/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4116 - mae: 0.3009 - val_loss: 0.3215 - val_mae: 0.2226 - learning_rate: 1.2500e-04\n",
      "Epoch 72/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3934 - mae: 0.2904 - val_loss: 0.3211 - val_mae: 0.2230 - learning_rate: 1.2500e-04\n",
      "Epoch 73/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4045 - mae: 0.2984 - val_loss: 0.3284 - val_mae: 0.2292 - learning_rate: 1.2500e-04\n",
      "Epoch 74/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3872 - mae: 0.2899 - val_loss: 0.3250 - val_mae: 0.2259 - learning_rate: 1.2500e-04\n",
      "Epoch 75/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4038 - mae: 0.2964 - val_loss: 0.3221 - val_mae: 0.2231 - learning_rate: 1.2500e-04\n",
      "Epoch 76/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4090 - mae: 0.3001 - val_loss: 0.3210 - val_mae: 0.2216 - learning_rate: 1.2500e-04\n",
      "Epoch 77/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4130 - mae: 0.2968 - val_loss: 0.3214 - val_mae: 0.2231 - learning_rate: 1.2500e-04\n",
      "Epoch 78/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4089 - mae: 0.2952 - val_loss: 0.3193 - val_mae: 0.2206 - learning_rate: 1.2500e-04\n",
      "Epoch 79/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3973 - mae: 0.2933 - val_loss: 0.3208 - val_mae: 0.2226 - learning_rate: 1.2500e-04\n",
      "Epoch 80/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4254 - mae: 0.3014 - val_loss: 0.3216 - val_mae: 0.2227 - learning_rate: 1.2500e-04\n",
      "Epoch 81/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4192 - mae: 0.3008 - val_loss: 0.3241 - val_mae: 0.2260 - learning_rate: 1.2500e-04\n",
      "Epoch 82/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4046 - mae: 0.2957 - val_loss: 0.3178 - val_mae: 0.2193 - learning_rate: 1.2500e-04\n",
      "Epoch 83/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4042 - mae: 0.2919 - val_loss: 0.3196 - val_mae: 0.2217 - learning_rate: 1.2500e-04\n",
      "Epoch 84/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4040 - mae: 0.2957 - val_loss: 0.3191 - val_mae: 0.2215 - learning_rate: 1.2500e-04\n",
      "Epoch 85/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3983 - mae: 0.2939 - val_loss: 0.3210 - val_mae: 0.2231 - learning_rate: 1.2500e-04\n",
      "Epoch 86/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4012 - mae: 0.2928 - val_loss: 0.3181 - val_mae: 0.2204 - learning_rate: 1.2500e-04\n",
      "Epoch 87/250\n",
      "\u001b[1m 88/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3974 - mae: 0.2956\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3979 - mae: 0.2949 - val_loss: 0.3221 - val_mae: 0.2242 - learning_rate: 1.2500e-04\n",
      "Epoch 88/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3937 - mae: 0.2875 - val_loss: 0.3151 - val_mae: 0.2198 - learning_rate: 6.2500e-05\n",
      "Epoch 89/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4043 - mae: 0.2966 - val_loss: 0.3214 - val_mae: 0.2271 - learning_rate: 6.2500e-05\n",
      "Epoch 90/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3937 - mae: 0.2940 - val_loss: 0.3193 - val_mae: 0.2248 - learning_rate: 6.2500e-05\n",
      "Epoch 91/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4004 - mae: 0.2961 - val_loss: 0.3169 - val_mae: 0.2224 - learning_rate: 6.2500e-05\n",
      "Epoch 92/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3993 - mae: 0.2937 - val_loss: 0.3159 - val_mae: 0.2216 - learning_rate: 6.2500e-05\n",
      "Epoch 93/250\n",
      "\u001b[1m 91/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4275 - mae: 0.3107\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4212 - mae: 0.3066 - val_loss: 0.3164 - val_mae: 0.2224 - learning_rate: 6.2500e-05\n",
      "Epoch 94/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3915 - mae: 0.2954 - val_loss: 0.3146 - val_mae: 0.2222 - learning_rate: 3.1250e-05\n",
      "Epoch 95/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3817 - mae: 0.2925 - val_loss: 0.3146 - val_mae: 0.2227 - learning_rate: 3.1250e-05\n",
      "Epoch 96/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4079 - mae: 0.3013 - val_loss: 0.3134 - val_mae: 0.2213 - learning_rate: 3.1250e-05\n",
      "Epoch 97/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3941 - mae: 0.2953 - val_loss: 0.3142 - val_mae: 0.2225 - learning_rate: 3.1250e-05\n",
      "Epoch 98/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4057 - mae: 0.2973 - val_loss: 0.3134 - val_mae: 0.2213 - learning_rate: 3.1250e-05\n",
      "Epoch 99/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3965 - mae: 0.2981 - val_loss: 0.3146 - val_mae: 0.2227 - learning_rate: 3.1250e-05\n",
      "Epoch 100/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3956 - mae: 0.2936 - val_loss: 0.3128 - val_mae: 0.2207 - learning_rate: 3.1250e-05\n",
      "Epoch 101/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4058 - mae: 0.2976 - val_loss: 0.3141 - val_mae: 0.2220 - learning_rate: 3.1250e-05\n",
      "Epoch 102/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3860 - mae: 0.2895 - val_loss: 0.3136 - val_mae: 0.2222 - learning_rate: 3.1250e-05\n",
      "Epoch 103/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4026 - mae: 0.2992 - val_loss: 0.3137 - val_mae: 0.2223 - learning_rate: 3.1250e-05\n",
      "Epoch 104/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3893 - mae: 0.2941 - val_loss: 0.3130 - val_mae: 0.2215 - learning_rate: 3.1250e-05\n",
      "Epoch 105/250\n",
      "\u001b[1m 92/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4148 - mae: 0.3007\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4138 - mae: 0.3001 - val_loss: 0.3144 - val_mae: 0.2232 - learning_rate: 3.1250e-05\n",
      "Epoch 106/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3794 - mae: 0.2871 - val_loss: 0.3132 - val_mae: 0.2227 - learning_rate: 1.5625e-05\n",
      "Epoch 107/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3993 - mae: 0.2933 - val_loss: 0.3131 - val_mae: 0.2227 - learning_rate: 1.5625e-05\n",
      "Epoch 108/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3855 - mae: 0.2925 - val_loss: 0.3129 - val_mae: 0.2226 - learning_rate: 1.5625e-05\n",
      "Epoch 109/250\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3889 - mae: 0.2940 - val_loss: 0.3137 - val_mae: 0.2235 - learning_rate: 1.5625e-05\n",
      "Epoch 110/250\n",
      "\u001b[1m 87/125\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3927 - mae: 0.2960\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3935 - mae: 0.2956 - val_loss: 0.3141 - val_mae: 0.2238 - learning_rate: 1.5625e-05\n"
     ]
    }
   ],
   "source": [
    "# TRAIN IT\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=250, batch_size=64,\n",
    "                    validation_split=0.2, verbose=1, callbacks=[\n",
    "                        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "                        # Puedes agregar más callbacks como TensorBoard si es necesario\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step \n",
      "============================================================\n",
      "Métricas del Modelo en el Conjunto de Prueba:\n",
      "Mean Absolute Error (MAE): 14065.98\n",
      "Mean Squared Error (MSE): 449752016.09\n",
      "Root Mean Squared Error (RMSE): 21207.36\n",
      "Root Mean Squared Error (RMSE en %): 11.73%\n",
      "R-squared (R2): 0.8920\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Invertir la escala para obtener las predicciones en la escala original\n",
    "y_pred = price_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = price_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Calcular las métricas\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "mean_y_true = np.mean(y_true)\n",
    "rmse_percentage = (rmse / mean_y_true) * 100\n",
    "mse_percentage = (mse / mean_y_true) * 100\n",
    "\n",
    "print(\"============================================================\")\n",
    "print(\"Métricas del Modelo en el Conjunto de Prueba:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "# print(f\"Mean Squared Error (MSE en %): {mse_percentage:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE en %): {rmse_percentage:.2f}%\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "print(\"============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"model-v1.0.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
