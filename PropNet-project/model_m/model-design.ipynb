{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design: Neutal Network creation with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LIBS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Lion\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ocultar advertencias de TensorFlow y otros warnings innecesarios\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Oculta logs de TensorFlow (0 = todos, 1 = INFO, 2 = WARNING, 3 = ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Oculta warnings de Python\n",
    "\n",
    "# Desactivar ejecución ansiosa (si no la necesitas)\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "PRICE = 'SalePrice'\n",
    "# PRICE = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records (rows): 1460\n",
      "Number of columns: 231\n",
      "\n",
      "Column names:\n",
      "['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'ExterQual_TA', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'KitchenQual_TA', 'YearRemodAdd', 'Foundation_PConc', 'MasVnrArea', 'Fireplaces', 'ExterQual_Gd', 'BsmtQual_TA', 'BsmtFinType1_GLQ', 'Neighborhood_NridgHt', 'BsmtFinSF1', 'SaleType_New', 'SaleCondition_Partial', 'Foundation_CBlock', 'Neighborhood_NoRidge', 'WoodDeckSF', 'KitchenQual_Gd', '2ndFlrSF', 'OpenPorchSF', 'HeatingQC_TA', 'BsmtExposure_Gd', 'Exterior2nd_VinylSd', 'Exterior1st_VinylSd', 'MSZoning_RM', 'HalfBath', 'LotShape_Reg', 'LotArea', 'BsmtExposure_No', 'CentralAir', 'MSZoning_RL', 'HouseStyle_2Story', 'SaleType_WD', 'Electrical_SBrkr', 'RoofStyle_Hip', 'BsmtQual_Gd', 'BsmtFullBath', 'RoofStyle_Gable', 'Neighborhood_StoneBr', 'BsmtUnfSF', 'PavedDrive', 'Neighborhood_OldTown', 'Neighborhood_NAmes', 'Neighborhood_Edwards', 'RoofMatl_WdShngl', 'BedroomAbvGr', 'Exterior1st_MetalSd', 'Neighborhood_IDOTRR', 'Exterior2nd_MetalSd', 'Exterior2nd_Wd Sdng', 'Exterior1st_Wd Sdng', 'KitchenQual_Fa', 'SaleCondition_Normal', 'Neighborhood_BrkSide', 'LotConfig_CulDSac', 'Neighborhood_Somerst', 'ExterCond_Fa', 'KitchenAbvGr', 'BsmtFinType1_Rec', 'HeatingQC_Gd', 'HeatingQC_Fa', 'Exterior1st_CemntBd', 'BsmtFinType1_BLQ', 'BsmtQual_Fa', 'EnclosedPorch', 'Neighborhood_Sawyer', 'Exterior2nd_CmentBd', 'Electrical_FuseF', 'Neighborhood_Timber', 'LotShape_IR2', 'LandContour_HLS', 'Foundation_Slab', 'BsmtFinType2_Unf', 'Condition1_Feedr', 'Functional_Typ', 'ExterQual_Fa', 'BldgType_Duplex', 'Condition1_Norm', 'Neighborhood_MeadowV', 'ScreenPorch', 'ExterCond_TA', 'RoofMatl_CompShg', 'Neighborhood_BrDale', 'BsmtCond_TA', 'BldgType_Twnhs', 'BldgType_2fmCon', 'Exterior1st_HdBoard', 'HouseStyle_SFoyer', 'Heating_GasA', 'PoolArea', 'Heating_Grav', 'MSZoning_FV', 'BsmtCond_Gd', 'HouseStyle_1.5Unf', 'BsmtFinType1_LwQ', 'MSSubClass', 'BsmtFinType1_Unf', 'LotConfig_Inside', 'OverallCond', 'Exterior2nd_ImStucc', 'Neighborhood_CollgCr', 'Functional_Min2', 'Neighborhood_Crawfor', 'Functional_Maj2', 'Exterior2nd_HdBoard', 'MSZoning_RH', 'Functional_Min1', 'Neighborhood_SWISU', 'Neighborhood_Veenker', 'HouseStyle_1Story', 'Heating_Wall', 'Neighborhood_Mitchel', 'BsmtFinType2_BLQ', 'Neighborhood_ClearCr', 'BsmtCond_Po', 'Exterior2nd_Plywood', 'Exterior1st_WdShing', 'Exterior1st_BrkComm', 'SaleCondition_AdjLand', 'ExterCond_Gd', 'Condition1_PosN', 'Condition2_PosN', 'Condition2_Feedr', 'Electrical_FuseP', 'Condition2_PosA', 'Exterior2nd_Brk Cmn', 'Condition1_RRAe', 'SaleCondition_Family', 'MoSold', 'LandContour_Low', 'Exterior2nd_Other', 'RoofMatl_WdShake', '3SsnPorch', 'BsmtExposure_Mn', 'LandSlope_Mod', 'Exterior2nd_Stucco', 'Condition1_PosA', 'SaleType_ConLD', 'SaleType_Con', 'Street_Pave', 'Exterior2nd_Wd Shng', 'BsmtFinType2_Rec', 'Condition2_RRNn', 'HouseStyle_SLvl', 'Neighborhood_NPkVill', 'BsmtFinType2_LwQ', 'Electrical_Mix', 'LotShape_IR3', 'HouseStyle_2.5Fin', 'Exterior1st_Stone', 'Neighborhood_Gilbert', 'RoofStyle_Gambrel', 'SaleType_Oth', 'ExterCond_Po', 'Exterior1st_BrkFace', 'HeatingQC_Po', 'Condition2_Norm', 'Exterior1st_Stucco', 'YrSold', 'LandSlope_Sev', 'LandContour_Lvl', 'SaleType_ConLw', 'Exterior1st_ImStucc', 'Exterior1st_AsphShn', 'HouseStyle_2.5Unf', 'Heating_OthW', 'LowQualFinSF', 'Exterior1st_CBlock', 'Exterior2nd_CBlock', 'Exterior2nd_BrkFace', 'Exterior2nd_AsphShn', 'Neighborhood_NWAmes', 'Condition1_RRNn', 'Id', 'MiscVal', 'RoofStyle_Shed', 'Neighborhood_Blueste', 'Heating_GasW', 'RoofMatl_Membran', 'SaleType_CWD', 'LotConfig_FR3', 'Exterior1st_Plywood', 'KPI_mult_Id_ExterQual_Gd', 'KPI_mult_Id_BsmtFinType1_GLQ', 'KPI_mult_Id_BsmtQual_TA', 'KPI_mult_Id_Neighborhood_NridgHt', 'KPI_mult_Id_Neighborhood_NoRidge', 'KPI_mult_Id_SaleType_New', 'KPI_mult_Id_SaleCondition_Partial', 'KPI_mult_Id_BsmtFinSF1', 'KPI_mult_Id_Foundation_CBlock', 'KPI_mult_Id_BsmtExposure_Gd', 'KPI_mult_Id_HeatingQC_TA', 'KPI_mult_Id_WoodDeckSF', 'KPI_mult_Id_OpenPorchSF', 'KPI_mult_Id_KitchenQual_Gd', 'KPI_mult_Id_2ndFlrSF', 'KPI_mult_Id_MSZoning_RM', 'KPI_mult_Id_Exterior2nd_VinylSd', 'KPI_mult_Id_Exterior1st_VinylSd', 'KPI_mult_Id_LotShape_Reg', 'KPI_mult_Id_LotArea', 'KPI_div_Id_LotArea', 'KPI_mult_Id_HalfBath', 'KPI_mult_Id_RoofStyle_Hip', 'KPI_mult_Id_BsmtExposure_No', 'KPI_mult_Id_BsmtQual_Gd', 'KPI_mult_Id_HouseStyle_2Story', 'KPI_div_Id_Neighborhood_StoneBr', 'KPI_mult_Id_RoofStyle_Gable', 'KPI_mult_Id_Neighborhood_StoneBr', 'KPI_mult_Id_BsmtFullBath', 'KPI_mult_Id_Neighborhood_OldTown', 'KPI_mult_Id_Neighborhood_NAmes', 'KPI_div_Id_Neighborhood_NridgHt', 'KPI_mult_Id_Neighborhood_Edwards', 'KPI_mult_Id_BsmtUnfSF', 'KPI_mult_Id_Exterior1st_MetalSd', 'KPI_mult_Id_SaleType_WD', 'KPI_div_Id_SaleType_New', 'KPI_mult_Id_Exterior2nd_Wd Sdng', 'KPI_div_Id_SaleCondition_Partial']\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE DATASET FROM PREVIOUS CSV\n",
    "# path = '/home/mike/Escritorio/codes/projects/POLARAI/neural-nets/data_m/csv_files/processed_data.csv'\n",
    "path = '/home/mike/Escritorio/codes/projects/POLARAI/neural-nets/data_m/csv_files/03_kpi_data.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Get number of rows and columns\n",
    "num_rows, num_cols = data.shape\n",
    "\n",
    "print(f\"Number of records (rows): {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\")\n",
    "\n",
    "# Optional: Display column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[PRICE])\n",
    "y = data[PRICE] \n",
    "\n",
    "# First check for and handle infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Then handle NaN values\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE THE DATA (MIN-MAX 1,0)\n",
    "price_scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "all_scaled_price = price_scaler.fit_transform(y.to_numpy().reshape(-1, 1)) \n",
    "all_scaled_vars = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_scaled_vars, all_scaled_price, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 230)\n",
      "(1168, 1)\n",
      "(292, 230)\n",
      "(292, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "y_train_scaled = y_train\n",
    "y_test_scaled = y_test\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL DEFINITION FUNCTION\n",
    "\n",
    "def build_model(optimizer_name='SGD'):\n",
    "    optimizers_dict = {\n",
    "        'Adam': Adam(learning_rate=0.001),\n",
    "        'SGD': SGD(learning_rate=0.001, momentum=0.9),\n",
    "        'RMSprop': RMSprop(learning_rate=0.001),\n",
    "        'Adagrad': Adagrad(learning_rate=0.001),\n",
    "        'Adadelta': Adadelta(learning_rate=1.0),\n",
    "        'Adamax': Adamax(learning_rate=0.002),\n",
    "        'Nadam': Nadam(learning_rate=0.001),\n",
    "        'Lion' : Lion(learning_rate=0.001)\n",
    "    }\n",
    "\n",
    "    if optimizer_name not in optimizers_dict:\n",
    "        raise ValueError(f\"Optimizador {optimizer_name} no reconocido. Opciones válidas: {list(optimizers_dict.keys())}\")\n",
    "\n",
    "    optimizer = optimizers_dict[optimizer_name]\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout aumentado\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout aumentado\n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout en cada capa\n",
    "        layers.Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.1),  # Dropout en cada capa\n",
    "        layers.Dense(1, activation='linear')  # Capa de salida\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-FOLDING TO SELECT BEST OPTIMIZER\n",
    "\n",
    "# # Definir validación cruzada\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Lista de optimizadores a probar\n",
    "# optimizers = ['Adam', 'SGD', 'Adadelta', 'Lion']\n",
    "\n",
    "# # Diccionario para almacenar resultados\n",
    "# results = {}\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "# for opt_name in optimizers:\n",
    "#     fold_mae = []\n",
    "\n",
    "#     print(f\"Entrenando con el optimizador: {opt_name}\")\n",
    "\n",
    "#     for train_idx, val_idx in kfold.split(X_train_scaled):\n",
    "#         X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "#         y_train_fold, y_val_fold = y_train_scaled[train_idx], y_train_scaled[val_idx]\n",
    "\n",
    "#         try:\n",
    "#             # Crear un nuevo modelo con el optimizador especificado\n",
    "#             model = build_model(optimizer_name=opt_name)\n",
    "\n",
    "#             # Entrenar el modelo\n",
    "#             history = model.fit(X_train_fold, y_train_fold, epochs=30, batch_size=24,\n",
    "#                                         validation_data=(X_val_fold, y_val_fold), verbose=0,\n",
    "#                                         callbacks=[early_stopping])\n",
    "\n",
    "#             # Evaluar el modelo\n",
    "#             val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)[1]\n",
    "#             fold_mae.append(val_mae)\n",
    "\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Error con optimizador {opt_name}: {e}\")\n",
    "#             continue  # Saltar a la siguiente iteración si hay un error\n",
    "\n",
    "#     # Guardar el MAE promedio en los resultados solo si hubo evaluaciones válidas\n",
    "#     if fold_mae:\n",
    "#         results[opt_name] = np.mean(fold_mae)\n",
    "#     else:\n",
    "#         results[opt_name] = float('inf')  # Asignar un valor alto si no se pudo entrenar\n",
    "\n",
    "# print('============================================================')\n",
    "# # Imprimir los resultados de validación cruzada\n",
    "# print(\"\\nResultados de Validación Cruzada (MAE promedio):\")\n",
    "# for opt, mae in results.items():\n",
    "#     print(f\"{opt}: {mae:.4f}\")\n",
    "# print('============================================================')\n",
    "# # Seleccionar el mejor optimizador\n",
    "# best_optimizer_name = min(results, key=results.get)\n",
    "# print(f\"\\nMejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "# print('============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor optimizador seleccionado: Adam\n"
     ]
    }
   ],
   "source": [
    "# CREATE THE MODEL AND\n",
    "# best_optimizer_name = min(results, key=results.get)\n",
    "best_optimizer_name = 'Adam'\n",
    "print(f\"Mejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "\n",
    "model = build_model(optimizer_name=best_optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - loss: 32.4926 - mae: 0.6268 - val_loss: 28.6884 - val_mae: 0.4435 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.7759 - mae: 0.4800 - val_loss: 24.4175 - val_mae: 0.3406 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.5894 - mae: 0.4292 - val_loss: 20.4984 - val_mae: 0.3066 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7512 - mae: 0.3752 - val_loss: 16.9678 - val_mae: 0.2854 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.2792 - mae: 0.3535 - val_loss: 13.8288 - val_mae: 0.2751 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.2236 - mae: 0.3325 - val_loss: 11.0961 - val_mae: 0.2622 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.6636 - mae: 0.3318 - val_loss: 8.8076 - val_mae: 0.2553 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.4629 - mae: 0.3252 - val_loss: 6.9339 - val_mae: 0.2486 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6514 - mae: 0.3060 - val_loss: 5.5056 - val_mae: 0.2541 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4784 - mae: 0.3623 - val_loss: 4.4778 - val_mae: 0.2585 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3562 - mae: 0.3292 - val_loss: 3.6662 - val_mae: 0.2623 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5921 - mae: 0.3157 - val_loss: 3.0522 - val_mae: 0.2533 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0362 - mae: 0.3315 - val_loss: 2.5587 - val_mae: 0.2522 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5718 - mae: 0.3153 - val_loss: 2.1840 - val_mae: 0.2478 - learning_rate: 0.0010\n",
      "Epoch 15/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2695 - mae: 0.3425 - val_loss: 1.8908 - val_mae: 0.2405 - learning_rate: 0.0010\n",
      "Epoch 16/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9022 - mae: 0.3118 - val_loss: 1.6454 - val_mae: 0.2362 - learning_rate: 0.0010\n",
      "Epoch 17/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7641 - mae: 0.3318 - val_loss: 1.4657 - val_mae: 0.2397 - learning_rate: 0.0010\n",
      "Epoch 18/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5790 - mae: 0.3214 - val_loss: 1.3239 - val_mae: 0.2437 - learning_rate: 0.0010\n",
      "Epoch 19/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3944 - mae: 0.3056 - val_loss: 1.1961 - val_mae: 0.2419 - learning_rate: 0.0010\n",
      "Epoch 20/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3189 - mae: 0.3153 - val_loss: 1.0930 - val_mae: 0.2404 - learning_rate: 0.0010\n",
      "Epoch 21/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1851 - mae: 0.3195 - val_loss: 1.0069 - val_mae: 0.2365 - learning_rate: 0.0010\n",
      "Epoch 22/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1086 - mae: 0.3237 - val_loss: 0.9374 - val_mae: 0.2423 - learning_rate: 0.0010\n",
      "Epoch 23/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0839 - mae: 0.3275 - val_loss: 0.8787 - val_mae: 0.2452 - learning_rate: 0.0010\n",
      "Epoch 24/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9983 - mae: 0.3174 - val_loss: 0.8348 - val_mae: 0.2440 - learning_rate: 0.0010\n",
      "Epoch 25/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9443 - mae: 0.3386 - val_loss: 0.7882 - val_mae: 0.2392 - learning_rate: 0.0010\n",
      "Epoch 26/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9564 - mae: 0.3467 - val_loss: 0.7501 - val_mae: 0.2428 - learning_rate: 0.0010\n",
      "Epoch 27/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9190 - mae: 0.3410 - val_loss: 0.7152 - val_mae: 0.2429 - learning_rate: 0.0010\n",
      "Epoch 28/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8300 - mae: 0.3303 - val_loss: 0.6958 - val_mae: 0.2460 - learning_rate: 0.0010\n",
      "Epoch 29/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8001 - mae: 0.3230 - val_loss: 0.6676 - val_mae: 0.2427 - learning_rate: 0.0010\n",
      "Epoch 30/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7597 - mae: 0.3238 - val_loss: 0.6467 - val_mae: 0.2453 - learning_rate: 0.0010\n",
      "Epoch 31/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7308 - mae: 0.3202 - val_loss: 0.6267 - val_mae: 0.2394 - learning_rate: 0.0010\n",
      "Epoch 32/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7583 - mae: 0.3368 - val_loss: 0.6200 - val_mae: 0.2488 - learning_rate: 0.0010\n",
      "Epoch 33/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7245 - mae: 0.3182 - val_loss: 0.5862 - val_mae: 0.2422 - learning_rate: 0.0010\n",
      "Epoch 34/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7147 - mae: 0.3251 - val_loss: 0.5655 - val_mae: 0.2392 - learning_rate: 0.0010\n",
      "Epoch 35/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6702 - mae: 0.3255 - val_loss: 0.5549 - val_mae: 0.2431 - learning_rate: 0.0010\n",
      "Epoch 36/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6797 - mae: 0.3272 - val_loss: 0.5459 - val_mae: 0.2464 - learning_rate: 0.0010\n",
      "Epoch 37/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6244 - mae: 0.3074 - val_loss: 0.5346 - val_mae: 0.2485 - learning_rate: 0.0010\n",
      "Epoch 38/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6174 - mae: 0.3123 - val_loss: 0.5309 - val_mae: 0.2546 - learning_rate: 0.0010\n",
      "Epoch 39/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6516 - mae: 0.3158 - val_loss: 0.5078 - val_mae: 0.2403 - learning_rate: 0.0010\n",
      "Epoch 40/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5917 - mae: 0.3212 - val_loss: 0.4964 - val_mae: 0.2419 - learning_rate: 0.0010\n",
      "Epoch 41/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6312 - mae: 0.3334 - val_loss: 0.4935 - val_mae: 0.2453 - learning_rate: 0.0010\n",
      "Epoch 42/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6095 - mae: 0.3278 - val_loss: 0.4878 - val_mae: 0.2510 - learning_rate: 0.0010\n",
      "Epoch 43/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5542 - mae: 0.3153 - val_loss: 0.4699 - val_mae: 0.2454 - learning_rate: 0.0010\n",
      "Epoch 44/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6233 - mae: 0.3413 - val_loss: 0.4691 - val_mae: 0.2536 - learning_rate: 0.0010\n",
      "Epoch 45/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6004 - mae: 0.3222 - val_loss: 0.4773 - val_mae: 0.2562 - learning_rate: 0.0010\n",
      "Epoch 46/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5902 - mae: 0.3288 - val_loss: 0.4530 - val_mae: 0.2449 - learning_rate: 0.0010\n",
      "Epoch 47/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5724 - mae: 0.3395 - val_loss: 0.4506 - val_mae: 0.2437 - learning_rate: 0.0010\n",
      "Epoch 48/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5431 - mae: 0.3106 - val_loss: 0.4416 - val_mae: 0.2415 - learning_rate: 0.0010\n",
      "Epoch 49/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5126 - mae: 0.3122 - val_loss: 0.4351 - val_mae: 0.2441 - learning_rate: 0.0010\n",
      "Epoch 50/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5221 - mae: 0.3156 - val_loss: 0.4352 - val_mae: 0.2427 - learning_rate: 0.0010\n",
      "Epoch 51/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5252 - mae: 0.3280 - val_loss: 0.4292 - val_mae: 0.2346 - learning_rate: 0.0010\n",
      "Epoch 52/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5517 - mae: 0.3200 - val_loss: 0.4397 - val_mae: 0.2457 - learning_rate: 0.0010\n",
      "Epoch 53/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5197 - mae: 0.3155 - val_loss: 0.4341 - val_mae: 0.2428 - learning_rate: 0.0010\n",
      "Epoch 54/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4981 - mae: 0.3018 - val_loss: 0.4269 - val_mae: 0.2445 - learning_rate: 0.0010\n",
      "Epoch 55/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4904 - mae: 0.3090 - val_loss: 0.4273 - val_mae: 0.2455 - learning_rate: 0.0010\n",
      "Epoch 56/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4856 - mae: 0.3120 - val_loss: 0.4233 - val_mae: 0.2393 - learning_rate: 0.0010\n",
      "Epoch 57/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4838 - mae: 0.2985 - val_loss: 0.4209 - val_mae: 0.2425 - learning_rate: 0.0010\n",
      "Epoch 58/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5526 - mae: 0.3261 - val_loss: 0.4318 - val_mae: 0.2474 - learning_rate: 0.0010\n",
      "Epoch 59/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5288 - mae: 0.3245 - val_loss: 0.4168 - val_mae: 0.2439 - learning_rate: 0.0010\n",
      "Epoch 60/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5473 - mae: 0.3357 - val_loss: 0.4152 - val_mae: 0.2404 - learning_rate: 0.0010\n",
      "Epoch 61/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5121 - mae: 0.3181 - val_loss: 0.4115 - val_mae: 0.2416 - learning_rate: 0.0010\n",
      "Epoch 62/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5078 - mae: 0.3204 - val_loss: 0.4099 - val_mae: 0.2392 - learning_rate: 0.0010\n",
      "Epoch 63/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4976 - mae: 0.3129 - val_loss: 0.4116 - val_mae: 0.2396 - learning_rate: 0.0010\n",
      "Epoch 64/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4851 - mae: 0.3065 - val_loss: 0.4073 - val_mae: 0.2365 - learning_rate: 0.0010\n",
      "Epoch 65/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4624 - mae: 0.2879 - val_loss: 0.4017 - val_mae: 0.2343 - learning_rate: 0.0010\n",
      "Epoch 66/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4663 - mae: 0.2956 - val_loss: 0.4093 - val_mae: 0.2409 - learning_rate: 0.0010\n",
      "Epoch 67/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5050 - mae: 0.3336 - val_loss: 0.4178 - val_mae: 0.2532 - learning_rate: 0.0010\n",
      "Epoch 68/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5374 - mae: 0.3242 - val_loss: 0.3961 - val_mae: 0.2314 - learning_rate: 0.0010\n",
      "Epoch 69/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4793 - mae: 0.3039 - val_loss: 0.3984 - val_mae: 0.2312 - learning_rate: 0.0010\n",
      "Epoch 70/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5336 - mae: 0.3387 - val_loss: 0.4031 - val_mae: 0.2346 - learning_rate: 0.0010\n",
      "Epoch 71/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5543 - mae: 0.3360 - val_loss: 0.3962 - val_mae: 0.2333 - learning_rate: 0.0010\n",
      "Epoch 72/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4725 - mae: 0.3058 - val_loss: 0.3953 - val_mae: 0.2307 - learning_rate: 0.0010\n",
      "Epoch 73/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5238 - mae: 0.3110 - val_loss: 0.3921 - val_mae: 0.2296 - learning_rate: 0.0010\n",
      "Epoch 74/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5481 - mae: 0.3351 - val_loss: 0.4039 - val_mae: 0.2406 - learning_rate: 0.0010\n",
      "Epoch 75/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4647 - mae: 0.3044 - val_loss: 0.3912 - val_mae: 0.2302 - learning_rate: 0.0010\n",
      "Epoch 76/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5682 - mae: 0.3335 - val_loss: 0.3945 - val_mae: 0.2316 - learning_rate: 0.0010\n",
      "Epoch 77/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5375 - mae: 0.3193 - val_loss: 0.3958 - val_mae: 0.2303 - learning_rate: 0.0010\n",
      "Epoch 78/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5567 - mae: 0.3422 - val_loss: 0.3885 - val_mae: 0.2259 - learning_rate: 0.0010\n",
      "Epoch 79/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5401 - mae: 0.3376 - val_loss: 0.3878 - val_mae: 0.2221 - learning_rate: 0.0010\n",
      "Epoch 80/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4896 - mae: 0.3192 - val_loss: 0.3900 - val_mae: 0.2282 - learning_rate: 0.0010\n",
      "Epoch 81/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4929 - mae: 0.3020 - val_loss: 0.3892 - val_mae: 0.2261 - learning_rate: 0.0010\n",
      "Epoch 82/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5139 - mae: 0.3201 - val_loss: 0.3885 - val_mae: 0.2295 - learning_rate: 0.0010\n",
      "Epoch 83/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4802 - mae: 0.3026 - val_loss: 0.3905 - val_mae: 0.2253 - learning_rate: 0.0010\n",
      "Epoch 84/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5562 - mae: 0.3447\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4922 - mae: 0.3076 - val_loss: 0.3916 - val_mae: 0.2283 - learning_rate: 0.0010\n",
      "Epoch 85/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4968 - mae: 0.3142 - val_loss: 0.3692 - val_mae: 0.2222 - learning_rate: 5.0000e-04\n",
      "Epoch 86/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4776 - mae: 0.3148 - val_loss: 0.3660 - val_mae: 0.2241 - learning_rate: 5.0000e-04\n",
      "Epoch 87/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4979 - mae: 0.3151 - val_loss: 0.3624 - val_mae: 0.2238 - learning_rate: 5.0000e-04\n",
      "Epoch 88/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5137 - mae: 0.3275 - val_loss: 0.3611 - val_mae: 0.2240 - learning_rate: 5.0000e-04\n",
      "Epoch 89/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4698 - mae: 0.3127 - val_loss: 0.3585 - val_mae: 0.2223 - learning_rate: 5.0000e-04\n",
      "Epoch 90/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4848 - mae: 0.3174 - val_loss: 0.3618 - val_mae: 0.2266 - learning_rate: 5.0000e-04\n",
      "Epoch 91/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4630 - mae: 0.3084 - val_loss: 0.3559 - val_mae: 0.2239 - learning_rate: 5.0000e-04\n",
      "Epoch 92/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4380 - mae: 0.2985 - val_loss: 0.3598 - val_mae: 0.2243 - learning_rate: 5.0000e-04\n",
      "Epoch 93/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4502 - mae: 0.3152 - val_loss: 0.3620 - val_mae: 0.2211 - learning_rate: 5.0000e-04\n",
      "Epoch 94/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4496 - mae: 0.2951 - val_loss: 0.3594 - val_mae: 0.2207 - learning_rate: 5.0000e-04\n",
      "Epoch 95/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4506 - mae: 0.3075 - val_loss: 0.3563 - val_mae: 0.2224 - learning_rate: 5.0000e-04\n",
      "Epoch 96/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7463 - mae: 0.3953\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5231 - mae: 0.3304 - val_loss: 0.3574 - val_mae: 0.2228 - learning_rate: 5.0000e-04\n",
      "Epoch 97/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4722 - mae: 0.3268 - val_loss: 0.3498 - val_mae: 0.2226 - learning_rate: 2.5000e-04\n",
      "Epoch 98/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4325 - mae: 0.3035 - val_loss: 0.3460 - val_mae: 0.2214 - learning_rate: 2.5000e-04\n",
      "Epoch 99/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4597 - mae: 0.3145 - val_loss: 0.3481 - val_mae: 0.2253 - learning_rate: 2.5000e-04\n",
      "Epoch 100/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4610 - mae: 0.3088 - val_loss: 0.3462 - val_mae: 0.2244 - learning_rate: 2.5000e-04\n",
      "Epoch 101/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4528 - mae: 0.3021 - val_loss: 0.3444 - val_mae: 0.2242 - learning_rate: 2.5000e-04\n",
      "Epoch 102/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4398 - mae: 0.2882 - val_loss: 0.3438 - val_mae: 0.2241 - learning_rate: 2.5000e-04\n",
      "Epoch 103/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4429 - mae: 0.3064 - val_loss: 0.3460 - val_mae: 0.2266 - learning_rate: 2.5000e-04\n",
      "Epoch 104/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4762 - mae: 0.3321 - val_loss: 0.3484 - val_mae: 0.2271 - learning_rate: 2.5000e-04\n",
      "Epoch 105/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5415 - mae: 0.3388 - val_loss: 0.3477 - val_mae: 0.2274 - learning_rate: 2.5000e-04\n",
      "Epoch 106/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4223 - mae: 0.3086 - val_loss: 0.3506 - val_mae: 0.2284 - learning_rate: 2.5000e-04\n",
      "Epoch 107/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5985 - mae: 0.3288\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4595 - mae: 0.3043 - val_loss: 0.3504 - val_mae: 0.2293 - learning_rate: 2.5000e-04\n",
      "Epoch 108/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4492 - mae: 0.2947 - val_loss: 0.3439 - val_mae: 0.2275 - learning_rate: 1.2500e-04\n",
      "Epoch 109/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4046 - mae: 0.2882 - val_loss: 0.3392 - val_mae: 0.2244 - learning_rate: 1.2500e-04\n",
      "Epoch 110/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4071 - mae: 0.2868 - val_loss: 0.3394 - val_mae: 0.2248 - learning_rate: 1.2500e-04\n",
      "Epoch 111/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3957 - mae: 0.2849 - val_loss: 0.3389 - val_mae: 0.2249 - learning_rate: 1.2500e-04\n",
      "Epoch 112/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4046 - mae: 0.2929 - val_loss: 0.3402 - val_mae: 0.2260 - learning_rate: 1.2500e-04\n",
      "Epoch 113/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3945 - mae: 0.2890 - val_loss: 0.3393 - val_mae: 0.2261 - learning_rate: 1.2500e-04\n",
      "Epoch 114/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4105 - mae: 0.3054 - val_loss: 0.3395 - val_mae: 0.2256 - learning_rate: 1.2500e-04\n",
      "Epoch 115/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4397 - mae: 0.3062 - val_loss: 0.3392 - val_mae: 0.2246 - learning_rate: 1.2500e-04\n",
      "Epoch 116/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4144 - mae: 0.2895 - val_loss: 0.3383 - val_mae: 0.2242 - learning_rate: 1.2500e-04\n",
      "Epoch 117/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4405 - mae: 0.3179 - val_loss: 0.3383 - val_mae: 0.2240 - learning_rate: 1.2500e-04\n",
      "Epoch 118/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4266 - mae: 0.3048 - val_loss: 0.3374 - val_mae: 0.2233 - learning_rate: 1.2500e-04\n",
      "Epoch 119/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4515 - mae: 0.3107 - val_loss: 0.3378 - val_mae: 0.2245 - learning_rate: 1.2500e-04\n",
      "Epoch 120/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4414 - mae: 0.3075 - val_loss: 0.3384 - val_mae: 0.2243 - learning_rate: 1.2500e-04\n",
      "Epoch 121/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4135 - mae: 0.3017 - val_loss: 0.3372 - val_mae: 0.2231 - learning_rate: 1.2500e-04\n",
      "Epoch 122/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4576 - mae: 0.3178 - val_loss: 0.3395 - val_mae: 0.2256 - learning_rate: 1.2500e-04\n",
      "Epoch 123/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4588 - mae: 0.3048 - val_loss: 0.3399 - val_mae: 0.2259 - learning_rate: 1.2500e-04\n",
      "Epoch 124/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4132 - mae: 0.2953 - val_loss: 0.3400 - val_mae: 0.2253 - learning_rate: 1.2500e-04\n",
      "Epoch 125/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4340 - mae: 0.3102 - val_loss: 0.3391 - val_mae: 0.2248 - learning_rate: 1.2500e-04\n",
      "Epoch 126/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3892 - mae: 0.2727\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4205 - mae: 0.2986 - val_loss: 0.3382 - val_mae: 0.2244 - learning_rate: 1.2500e-04\n",
      "Epoch 127/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4695 - mae: 0.3176 - val_loss: 0.3355 - val_mae: 0.2240 - learning_rate: 6.2500e-05\n",
      "Epoch 128/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4245 - mae: 0.2811 - val_loss: 0.3344 - val_mae: 0.2235 - learning_rate: 6.2500e-05\n",
      "Epoch 129/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4646 - mae: 0.3250 - val_loss: 0.3341 - val_mae: 0.2232 - learning_rate: 6.2500e-05\n",
      "Epoch 130/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4728 - mae: 0.3207 - val_loss: 0.3339 - val_mae: 0.2231 - learning_rate: 6.2500e-05\n",
      "Epoch 131/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4274 - mae: 0.3060 - val_loss: 0.3335 - val_mae: 0.2231 - learning_rate: 6.2500e-05\n",
      "Epoch 132/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4066 - mae: 0.2880 - val_loss: 0.3335 - val_mae: 0.2233 - learning_rate: 6.2500e-05\n",
      "Epoch 133/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4195 - mae: 0.3053 - val_loss: 0.3337 - val_mae: 0.2237 - learning_rate: 6.2500e-05\n",
      "Epoch 134/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4042 - mae: 0.2912 - val_loss: 0.3344 - val_mae: 0.2244 - learning_rate: 6.2500e-05\n",
      "Epoch 135/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4293 - mae: 0.3108 - val_loss: 0.3347 - val_mae: 0.2252 - learning_rate: 6.2500e-05\n",
      "Epoch 136/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8206 - mae: 0.4505\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4897 - mae: 0.3198 - val_loss: 0.3343 - val_mae: 0.2246 - learning_rate: 6.2500e-05\n",
      "Epoch 137/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4249 - mae: 0.3052 - val_loss: 0.3329 - val_mae: 0.2246 - learning_rate: 3.1250e-05\n",
      "Epoch 138/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4207 - mae: 0.3048 - val_loss: 0.3324 - val_mae: 0.2243 - learning_rate: 3.1250e-05\n",
      "Epoch 139/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4299 - mae: 0.3077 - val_loss: 0.3323 - val_mae: 0.2244 - learning_rate: 3.1250e-05\n",
      "Epoch 140/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3976 - mae: 0.2877 - val_loss: 0.3319 - val_mae: 0.2241 - learning_rate: 3.1250e-05\n",
      "Epoch 141/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4555 - mae: 0.3074 - val_loss: 0.3316 - val_mae: 0.2239 - learning_rate: 3.1250e-05\n",
      "Epoch 142/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4679 - mae: 0.3193 - val_loss: 0.3323 - val_mae: 0.2247 - learning_rate: 3.1250e-05\n",
      "Epoch 143/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4140 - mae: 0.3003 - val_loss: 0.3321 - val_mae: 0.2245 - learning_rate: 3.1250e-05\n",
      "Epoch 144/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4538 - mae: 0.3141 - val_loss: 0.3315 - val_mae: 0.2237 - learning_rate: 3.1250e-05\n",
      "Epoch 145/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4335 - mae: 0.3035 - val_loss: 0.3310 - val_mae: 0.2232 - learning_rate: 3.1250e-05\n",
      "Epoch 146/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4438 - mae: 0.3209 - val_loss: 0.3306 - val_mae: 0.2229 - learning_rate: 3.1250e-05\n",
      "Epoch 147/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3912 - mae: 0.2828 - val_loss: 0.3307 - val_mae: 0.2233 - learning_rate: 3.1250e-05\n",
      "Epoch 148/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4400 - mae: 0.3089 - val_loss: 0.3302 - val_mae: 0.2230 - learning_rate: 3.1250e-05\n",
      "Epoch 149/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4183 - mae: 0.2965 - val_loss: 0.3302 - val_mae: 0.2227 - learning_rate: 3.1250e-05\n",
      "Epoch 150/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4647 - mae: 0.3218 - val_loss: 0.3298 - val_mae: 0.2221 - learning_rate: 3.1250e-05\n",
      "Epoch 151/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4298 - mae: 0.3101 - val_loss: 0.3296 - val_mae: 0.2216 - learning_rate: 3.1250e-05\n",
      "Epoch 152/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4569 - mae: 0.3191 - val_loss: 0.3298 - val_mae: 0.2219 - learning_rate: 3.1250e-05\n",
      "Epoch 153/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4150 - mae: 0.2928 - val_loss: 0.3301 - val_mae: 0.2222 - learning_rate: 3.1250e-05\n",
      "Epoch 154/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4033 - mae: 0.2875 - val_loss: 0.3302 - val_mae: 0.2225 - learning_rate: 3.1250e-05\n",
      "Epoch 155/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4486 - mae: 0.3146 - val_loss: 0.3301 - val_mae: 0.2224 - learning_rate: 3.1250e-05\n",
      "Epoch 156/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3885 - mae: 0.2895\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4794 - mae: 0.3252 - val_loss: 0.3301 - val_mae: 0.2225 - learning_rate: 3.1250e-05\n",
      "Epoch 157/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4154 - mae: 0.2962 - val_loss: 0.3296 - val_mae: 0.2227 - learning_rate: 1.5625e-05\n",
      "Epoch 158/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4454 - mae: 0.2898 - val_loss: 0.3295 - val_mae: 0.2226 - learning_rate: 1.5625e-05\n",
      "Epoch 159/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5020 - mae: 0.3150 - val_loss: 0.3295 - val_mae: 0.2228 - learning_rate: 1.5625e-05\n",
      "Epoch 160/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4023 - mae: 0.2810 - val_loss: 0.3295 - val_mae: 0.2229 - learning_rate: 1.5625e-05\n",
      "Epoch 161/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4773 - mae: 0.3211 - val_loss: 0.3295 - val_mae: 0.2228 - learning_rate: 1.5625e-05\n",
      "Epoch 162/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5074 - mae: 0.3257 - val_loss: 0.3294 - val_mae: 0.2226 - learning_rate: 1.5625e-05\n",
      "Epoch 163/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4213 - mae: 0.2820\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4257 - mae: 0.2982 - val_loss: 0.3295 - val_mae: 0.2227 - learning_rate: 1.5625e-05\n",
      "Epoch 164/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4487 - mae: 0.3224 - val_loss: 0.3294 - val_mae: 0.2230 - learning_rate: 7.8125e-06\n",
      "Epoch 165/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4257 - mae: 0.3032 - val_loss: 0.3293 - val_mae: 0.2230 - learning_rate: 7.8125e-06\n",
      "Epoch 166/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4920 - mae: 0.3282 - val_loss: 0.3293 - val_mae: 0.2230 - learning_rate: 7.8125e-06\n",
      "Epoch 167/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3881 - mae: 0.2787 - val_loss: 0.3293 - val_mae: 0.2230 - learning_rate: 7.8125e-06\n",
      "Epoch 168/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4377 - mae: 0.3069 - val_loss: 0.3292 - val_mae: 0.2229 - learning_rate: 7.8125e-06\n",
      "Epoch 169/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4198 - mae: 0.3029 - val_loss: 0.3290 - val_mae: 0.2227 - learning_rate: 7.8125e-06\n",
      "Epoch 170/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4627 - mae: 0.3206 - val_loss: 0.3291 - val_mae: 0.2228 - learning_rate: 7.8125e-06\n",
      "Epoch 171/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4069 - mae: 0.2847 - val_loss: 0.3290 - val_mae: 0.2227 - learning_rate: 7.8125e-06\n",
      "Epoch 172/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4090 - mae: 0.2945 - val_loss: 0.3289 - val_mae: 0.2226 - learning_rate: 7.8125e-06\n",
      "Epoch 173/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4309 - mae: 0.3127 - val_loss: 0.3289 - val_mae: 0.2226 - learning_rate: 7.8125e-06\n",
      "Epoch 174/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4427 - mae: 0.3193 - val_loss: 0.3288 - val_mae: 0.2225 - learning_rate: 7.8125e-06\n",
      "Epoch 175/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3866 - mae: 0.2922 - val_loss: 0.3288 - val_mae: 0.2224 - learning_rate: 7.8125e-06\n",
      "Epoch 176/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4838 - mae: 0.3319 - val_loss: 0.3288 - val_mae: 0.2224 - learning_rate: 7.8125e-06\n",
      "Epoch 177/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4373 - mae: 0.3236 - val_loss: 0.3288 - val_mae: 0.2224 - learning_rate: 7.8125e-06\n",
      "Epoch 178/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4378 - mae: 0.3133 - val_loss: 0.3289 - val_mae: 0.2224 - learning_rate: 7.8125e-06\n",
      "Epoch 179/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3298 - mae: 0.2656\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4227 - mae: 0.3065 - val_loss: 0.3288 - val_mae: 0.2224 - learning_rate: 7.8125e-06\n",
      "Epoch 180/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4253 - mae: 0.3040 - val_loss: 0.3287 - val_mae: 0.2224 - learning_rate: 3.9063e-06\n",
      "Epoch 181/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4104 - mae: 0.2921 - val_loss: 0.3287 - val_mae: 0.2224 - learning_rate: 3.9063e-06\n",
      "Epoch 182/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4378 - mae: 0.3040 - val_loss: 0.3287 - val_mae: 0.2225 - learning_rate: 3.9063e-06\n",
      "Epoch 183/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4562 - mae: 0.3301 - val_loss: 0.3288 - val_mae: 0.2225 - learning_rate: 3.9063e-06\n",
      "Epoch 184/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4207 - mae: 0.3078 - val_loss: 0.3288 - val_mae: 0.2225 - learning_rate: 3.9063e-06\n",
      "Epoch 185/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3254 - mae: 0.2377\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3943 - mae: 0.2826 - val_loss: 0.3288 - val_mae: 0.2225 - learning_rate: 3.9063e-06\n",
      "Epoch 186/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4514 - mae: 0.3095 - val_loss: 0.3287 - val_mae: 0.2226 - learning_rate: 1.9531e-06\n",
      "Epoch 187/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4251 - mae: 0.3165 - val_loss: 0.3287 - val_mae: 0.2225 - learning_rate: 1.9531e-06\n",
      "Epoch 188/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3982 - mae: 0.2941 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.9531e-06\n",
      "Epoch 189/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4085 - mae: 0.3123 - val_loss: 0.3287 - val_mae: 0.2226 - learning_rate: 1.9531e-06\n",
      "Epoch 190/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4478 - mae: 0.3445\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4632 - mae: 0.3244 - val_loss: 0.3287 - val_mae: 0.2226 - learning_rate: 1.9531e-06\n",
      "Epoch 191/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4165 - mae: 0.2977 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.7656e-07\n",
      "Epoch 192/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4064 - mae: 0.3007 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.7656e-07\n",
      "Epoch 193/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4267 - mae: 0.3073 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.7656e-07\n",
      "Epoch 194/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4684 - mae: 0.3286 - val_loss: 0.3287 - val_mae: 0.2226 - learning_rate: 9.7656e-07\n",
      "Epoch 195/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4116 - mae: 0.2858\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4056 - mae: 0.2915 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.7656e-07\n",
      "Epoch 196/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4220 - mae: 0.2955 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.8828e-07\n",
      "Epoch 197/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4404 - mae: 0.3195 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.8828e-07\n",
      "Epoch 198/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4879 - mae: 0.3247 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.8828e-07\n",
      "Epoch 199/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4419 - mae: 0.3215 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.8828e-07\n",
      "Epoch 200/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3407 - mae: 0.2294\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3912 - mae: 0.2789 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.8828e-07\n",
      "Epoch 201/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4023 - mae: 0.2850 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 2.4414e-07\n",
      "Epoch 202/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4086 - mae: 0.3017 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 2.4414e-07\n",
      "Epoch 203/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4365 - mae: 0.3013 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 2.4414e-07\n",
      "Epoch 204/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4297 - mae: 0.3164 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 2.4414e-07\n",
      "Epoch 205/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3538 - mae: 0.2630\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4133 - mae: 0.2960 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 2.4414e-07\n",
      "Epoch 206/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4052 - mae: 0.2921 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.2207e-07\n",
      "Epoch 207/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3884 - mae: 0.2883 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.2207e-07\n",
      "Epoch 208/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3924 - mae: 0.2876 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.2207e-07\n",
      "Epoch 209/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4195 - mae: 0.3090 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.2207e-07\n",
      "Epoch 210/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5383 - mae: 0.3897\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4313 - mae: 0.3180 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.2207e-07\n",
      "Epoch 211/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3940 - mae: 0.2950 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 6.1035e-08\n",
      "Epoch 212/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4825 - mae: 0.3154 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 6.1035e-08\n",
      "Epoch 213/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4257 - mae: 0.3071 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 6.1035e-08\n",
      "Epoch 214/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4223 - mae: 0.3079 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 6.1035e-08\n",
      "Epoch 215/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4387 - mae: 0.3246\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4457 - mae: 0.3131 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 6.1035e-08\n",
      "Epoch 216/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4188 - mae: 0.3010 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.0518e-08\n",
      "Epoch 217/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4211 - mae: 0.3044 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.0518e-08\n",
      "Epoch 218/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4026 - mae: 0.3067 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.0518e-08\n",
      "Epoch 219/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4125 - mae: 0.3104 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.0518e-08\n",
      "Epoch 220/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3952 - mae: 0.2957\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4364 - mae: 0.3092 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.0518e-08\n",
      "Epoch 221/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4987 - mae: 0.3396 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.5259e-08\n",
      "Epoch 222/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4098 - mae: 0.3070 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.5259e-08\n",
      "Epoch 223/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4115 - mae: 0.2982 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.5259e-08\n",
      "Epoch 224/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4356 - mae: 0.3046 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.5259e-08\n",
      "Epoch 225/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3595 - mae: 0.2625\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4069 - mae: 0.3007 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.5259e-08\n",
      "Epoch 226/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5094 - mae: 0.3285 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 7.6294e-09\n",
      "Epoch 227/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4305 - mae: 0.3100 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 7.6294e-09\n",
      "Epoch 228/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4085 - mae: 0.2907 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 7.6294e-09\n",
      "Epoch 229/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4156 - mae: 0.2963 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 7.6294e-09\n",
      "Epoch 230/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5123 - mae: 0.3294\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4354 - mae: 0.3001 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 7.6294e-09\n",
      "Epoch 231/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4553 - mae: 0.3146 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.8147e-09\n",
      "Epoch 232/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4089 - mae: 0.2962 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.8147e-09\n",
      "Epoch 233/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4245 - mae: 0.3105 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.8147e-09\n",
      "Epoch 234/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4184 - mae: 0.2962 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.8147e-09\n",
      "Epoch 235/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3519 - mae: 0.2463\n",
      "Epoch 235: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3978 - mae: 0.2894 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 3.8147e-09\n",
      "Epoch 236/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4403 - mae: 0.3012 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.9073e-09\n",
      "Epoch 237/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4063 - mae: 0.2978 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.9073e-09\n",
      "Epoch 238/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4448 - mae: 0.3047 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.9073e-09\n",
      "Epoch 239/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4007 - mae: 0.2905 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.9073e-09\n",
      "Epoch 240/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3851 - mae: 0.2839\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4361 - mae: 0.3176 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 1.9073e-09\n",
      "Epoch 241/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4137 - mae: 0.3007 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.5367e-10\n",
      "Epoch 242/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4761 - mae: 0.3163 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.5367e-10\n",
      "Epoch 243/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4479 - mae: 0.3147 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.5367e-10\n",
      "Epoch 244/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4486 - mae: 0.3119 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.5367e-10\n",
      "Epoch 245/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3198 - mae: 0.2511\n",
      "Epoch 245: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3848 - mae: 0.2908 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 9.5367e-10\n",
      "Epoch 246/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4373 - mae: 0.3173 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.7684e-10\n",
      "Epoch 247/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4084 - mae: 0.2968 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.7684e-10\n",
      "Epoch 248/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3981 - mae: 0.2861 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.7684e-10\n",
      "Epoch 249/250\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4258 - mae: 0.2997 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.7684e-10\n",
      "Epoch 250/250\n",
      "\u001b[1m 1/15\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4052 - mae: 0.2878\n",
      "Epoch 250: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4083 - mae: 0.2975 - val_loss: 0.3286 - val_mae: 0.2226 - learning_rate: 4.7684e-10\n"
     ]
    }
   ],
   "source": [
    "# TRAIN IT\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=250, batch_size=64,\n",
    "                    validation_split=0.2, verbose=1, callbacks=[\n",
    "                        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "                        # Puedes agregar más callbacks como TensorBoard si es necesario\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "============================================================\n",
      "Métricas del Modelo en el Conjunto de Prueba:\n",
      "Mean Absolute Error (MAE): 21388.22\n",
      "Mean Squared Error (MSE): 1213154567.17\n",
      "Mean Squared Error (MSE en %): 678347.04\n",
      "Root Mean Squared Error (RMSE): 34830.37\n",
      "Root Mean Squared Error (RMSE en %): 19.48%\n",
      "R-squared (R2): 0.8418\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Invertir la escala para obtener las predicciones en la escala original\n",
    "y_pred = price_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = price_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Calcular las métricas\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "mean_y_true = np.mean(y_true)\n",
    "rmse_percentage = (rmse / mean_y_true) * 100\n",
    "mse_percentage = (mse / mean_y_true) * 100\n",
    "\n",
    "print(\"============================================================\")\n",
    "print(\"Métricas del Modelo en el Conjunto de Prueba:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE en %): {mse_percentage:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE en %): {rmse_percentage:.2f}%\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "print(\"============================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
