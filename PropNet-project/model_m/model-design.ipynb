{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design: Neutal Network creation with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LIBS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Lion\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ocultar advertencias de TensorFlow y otros warnings innecesarios\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Oculta logs de TensorFlow (0 = todos, 1 = INFO, 2 = WARNING, 3 = ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Oculta warnings de Python\n",
    "\n",
    "# Desactivar ejecución ansiosa (si no la necesitas)\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "PRICE = 'SalePrice'\n",
    "# PRICE = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records (rows): 1460\n",
      "Number of columns: 231\n",
      "\n",
      "Column names:\n",
      "['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'ExterQual_TA', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'KitchenQual_TA', 'YearRemodAdd', 'Foundation_PConc', 'MasVnrArea', 'Fireplaces', 'ExterQual_Gd', 'BsmtQual_TA', 'BsmtFinType1_GLQ', 'Neighborhood_NridgHt', 'BsmtFinSF1', 'SaleType_New', 'SaleCondition_Partial', 'Foundation_CBlock', 'Neighborhood_NoRidge', 'WoodDeckSF', 'KitchenQual_Gd', '2ndFlrSF', 'OpenPorchSF', 'HeatingQC_TA', 'BsmtExposure_Gd', 'Exterior2nd_VinylSd', 'Exterior1st_VinylSd', 'MSZoning_RM', 'HalfBath', 'LotShape_Reg', 'LotArea', 'BsmtExposure_No', 'CentralAir', 'MSZoning_RL', 'HouseStyle_2Story', 'SaleType_WD', 'Electrical_SBrkr', 'RoofStyle_Hip', 'BsmtQual_Gd', 'BsmtFullBath', 'RoofStyle_Gable', 'Neighborhood_StoneBr', 'BsmtUnfSF', 'PavedDrive', 'Neighborhood_OldTown', 'Neighborhood_NAmes', 'Neighborhood_Edwards', 'RoofMatl_WdShngl', 'BedroomAbvGr', 'Exterior1st_MetalSd', 'Neighborhood_IDOTRR', 'Exterior2nd_MetalSd', 'Exterior2nd_Wd Sdng', 'Exterior1st_Wd Sdng', 'KitchenQual_Fa', 'SaleCondition_Normal', 'Neighborhood_BrkSide', 'LotConfig_CulDSac', 'Neighborhood_Somerst', 'ExterCond_Fa', 'KitchenAbvGr', 'BsmtFinType1_Rec', 'HeatingQC_Gd', 'HeatingQC_Fa', 'Exterior1st_CemntBd', 'BsmtFinType1_BLQ', 'BsmtQual_Fa', 'EnclosedPorch', 'Neighborhood_Sawyer', 'Exterior2nd_CmentBd', 'Electrical_FuseF', 'Neighborhood_Timber', 'LotShape_IR2', 'LandContour_HLS', 'Foundation_Slab', 'BsmtFinType2_Unf', 'Condition1_Feedr', 'Functional_Typ', 'ExterQual_Fa', 'BldgType_Duplex', 'Condition1_Norm', 'Neighborhood_MeadowV', 'ScreenPorch', 'ExterCond_TA', 'RoofMatl_CompShg', 'Neighborhood_BrDale', 'BsmtCond_TA', 'BldgType_Twnhs', 'BldgType_2fmCon', 'Exterior1st_HdBoard', 'HouseStyle_SFoyer', 'Heating_GasA', 'PoolArea', 'Heating_Grav', 'MSZoning_FV', 'BsmtCond_Gd', 'HouseStyle_1.5Unf', 'BsmtFinType1_LwQ', 'MSSubClass', 'BsmtFinType1_Unf', 'LotConfig_Inside', 'OverallCond', 'Exterior2nd_ImStucc', 'Neighborhood_CollgCr', 'Functional_Min2', 'Neighborhood_Crawfor', 'Functional_Maj2', 'Exterior2nd_HdBoard', 'MSZoning_RH', 'Functional_Min1', 'Neighborhood_SWISU', 'Neighborhood_Veenker', 'HouseStyle_1Story', 'Heating_Wall', 'Neighborhood_Mitchel', 'BsmtFinType2_BLQ', 'Neighborhood_ClearCr', 'BsmtCond_Po', 'Exterior2nd_Plywood', 'Exterior1st_WdShing', 'Exterior1st_BrkComm', 'SaleCondition_AdjLand', 'ExterCond_Gd', 'Condition1_PosN', 'Condition2_PosN', 'Condition2_Feedr', 'Electrical_FuseP', 'Condition2_PosA', 'Exterior2nd_Brk Cmn', 'Condition1_RRAe', 'SaleCondition_Family', 'MoSold', 'LandContour_Low', 'Exterior2nd_Other', 'RoofMatl_WdShake', '3SsnPorch', 'BsmtExposure_Mn', 'LandSlope_Mod', 'Exterior2nd_Stucco', 'Condition1_PosA', 'SaleType_ConLD', 'SaleType_Con', 'Street_Pave', 'Exterior2nd_Wd Shng', 'BsmtFinType2_Rec', 'Condition2_RRNn', 'HouseStyle_SLvl', 'Neighborhood_NPkVill', 'BsmtFinType2_LwQ', 'Electrical_Mix', 'LotShape_IR3', 'HouseStyle_2.5Fin', 'Exterior1st_Stone', 'Neighborhood_Gilbert', 'RoofStyle_Gambrel', 'SaleType_Oth', 'ExterCond_Po', 'Exterior1st_BrkFace', 'HeatingQC_Po', 'Condition2_Norm', 'Exterior1st_Stucco', 'YrSold', 'LandSlope_Sev', 'LandContour_Lvl', 'SaleType_ConLw', 'Exterior1st_ImStucc', 'Exterior1st_AsphShn', 'HouseStyle_2.5Unf', 'Heating_OthW', 'LowQualFinSF', 'Exterior1st_CBlock', 'Exterior2nd_CBlock', 'Exterior2nd_BrkFace', 'Exterior2nd_AsphShn', 'Neighborhood_NWAmes', 'Condition1_RRNn', 'Id', 'MiscVal', 'RoofStyle_Shed', 'Neighborhood_Blueste', 'Heating_GasW', 'RoofMatl_Membran', 'SaleType_CWD', 'LotConfig_FR3', 'Exterior1st_Plywood', 'KPI_mult_Id_ExterQual_Gd', 'KPI_mult_Id_BsmtFinType1_GLQ', 'KPI_mult_Id_BsmtQual_TA', 'KPI_mult_Id_Neighborhood_NridgHt', 'KPI_mult_Id_Neighborhood_NoRidge', 'KPI_mult_Id_SaleType_New', 'KPI_mult_Id_SaleCondition_Partial', 'KPI_mult_Id_BsmtFinSF1', 'KPI_mult_Id_Foundation_CBlock', 'KPI_mult_Id_BsmtExposure_Gd', 'KPI_mult_Id_HeatingQC_TA', 'KPI_mult_Id_WoodDeckSF', 'KPI_mult_Id_OpenPorchSF', 'KPI_mult_Id_KitchenQual_Gd', 'KPI_mult_Id_2ndFlrSF', 'KPI_mult_Id_MSZoning_RM', 'KPI_mult_Id_Exterior2nd_VinylSd', 'KPI_mult_Id_Exterior1st_VinylSd', 'KPI_mult_Id_LotShape_Reg', 'KPI_mult_Id_LotArea', 'KPI_div_Id_LotArea', 'KPI_mult_Id_HalfBath', 'KPI_mult_Id_RoofStyle_Hip', 'KPI_mult_Id_BsmtExposure_No', 'KPI_mult_Id_BsmtQual_Gd', 'KPI_mult_Id_HouseStyle_2Story', 'KPI_div_Id_Neighborhood_StoneBr', 'KPI_mult_Id_RoofStyle_Gable', 'KPI_mult_Id_Neighborhood_StoneBr', 'KPI_mult_Id_BsmtFullBath', 'KPI_mult_Id_Neighborhood_OldTown', 'KPI_mult_Id_Neighborhood_NAmes', 'KPI_div_Id_Neighborhood_NridgHt', 'KPI_mult_Id_Neighborhood_Edwards', 'KPI_mult_Id_BsmtUnfSF', 'KPI_mult_Id_Exterior1st_MetalSd', 'KPI_mult_Id_SaleType_WD', 'KPI_div_Id_SaleType_New', 'KPI_mult_Id_Exterior2nd_Wd Sdng', 'KPI_div_Id_SaleCondition_Partial']\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE DATASET FROM PREVIOUS CSV\n",
    "# path = '/home/mike/Escritorio/codes/projects/POLARAI/neural-nets/data_m/csv_files/processed_data.csv'\n",
    "path = '/home/mike/Escritorio/codes/projects/POLARAI/neural-nets/data_m/csv_files/03_kpi_data.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Get number of rows and columns\n",
    "num_rows, num_cols = data.shape\n",
    "\n",
    "print(f\"Number of records (rows): {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\")\n",
    "\n",
    "# Optional: Display column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[PRICE])\n",
    "y = data[PRICE] \n",
    "\n",
    "# First check for and handle infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Then handle NaN values\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE THE DATA (MIN-MAX 1,0)\n",
    "price_scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "all_scaled_price = price_scaler.fit_transform(y.to_numpy().reshape(-1, 1)) \n",
    "all_scaled_vars = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_scaled_vars, all_scaled_price, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 230)\n",
      "(1168, 1)\n",
      "(292, 230)\n",
      "(292, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "y_train_scaled = y_train\n",
    "y_test_scaled = y_test\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL DEFINITION FUNCTION\n",
    "\n",
    "def build_model(optimizer_name='SGD'):\n",
    "    optimizers_dict = {\n",
    "        'Adam': Adam(learning_rate=0.001),\n",
    "        'SGD': SGD(learning_rate=0.001, momentum=0.9),\n",
    "        'RMSprop': RMSprop(learning_rate=0.001),\n",
    "        'Adagrad': Adagrad(learning_rate=0.001),\n",
    "        'Adadelta': Adadelta(learning_rate=1.0),\n",
    "        'Adamax': Adamax(learning_rate=0.002),\n",
    "        'Nadam': Nadam(learning_rate=0.001),\n",
    "        'Lion' : Lion(learning_rate=0.001)\n",
    "    }\n",
    "\n",
    "    if optimizer_name not in optimizers_dict:\n",
    "        raise ValueError(f\"Optimizador {optimizer_name} no reconocido. Opciones válidas: {list(optimizers_dict.keys())}\")\n",
    "\n",
    "    optimizer = optimizers_dict[optimizer_name]\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.4),  # Dropout aumentado\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.2),  # Dropout aumentado\n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dropout(0.2),  # Dropout en cada capa\n",
    "        layers.Dense(1)  # Capa de salida\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-FOLDING TO SELECT BEST OPTIMIZER\n",
    "\n",
    "# # Definir validación cruzada\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Lista de optimizadores a probar\n",
    "# optimizers = ['Adam', 'SGD', 'Adadelta', 'Lion']\n",
    "\n",
    "# # Diccionario para almacenar resultados\n",
    "# results = {}\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "# for opt_name in optimizers:\n",
    "#     fold_mae = []\n",
    "\n",
    "#     print(f\"Entrenando con el optimizador: {opt_name}\")\n",
    "\n",
    "#     for train_idx, val_idx in kfold.split(X_train_scaled):\n",
    "#         X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "#         y_train_fold, y_val_fold = y_train_scaled[train_idx], y_train_scaled[val_idx]\n",
    "\n",
    "#         try:\n",
    "#             # Crear un nuevo modelo con el optimizador especificado\n",
    "#             model = build_model(optimizer_name=opt_name)\n",
    "\n",
    "#             # Entrenar el modelo\n",
    "#             history = model.fit(X_train_fold, y_train_fold, epochs=30, batch_size=24,\n",
    "#                                         validation_data=(X_val_fold, y_val_fold), verbose=0,\n",
    "#                                         callbacks=[early_stopping])\n",
    "\n",
    "#             # Evaluar el modelo\n",
    "#             val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)[1]\n",
    "#             fold_mae.append(val_mae)\n",
    "\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Error con optimizador {opt_name}: {e}\")\n",
    "#             continue  # Saltar a la siguiente iteración si hay un error\n",
    "\n",
    "#     # Guardar el MAE promedio en los resultados solo si hubo evaluaciones válidas\n",
    "#     if fold_mae:\n",
    "#         results[opt_name] = np.mean(fold_mae)\n",
    "#     else:\n",
    "#         results[opt_name] = float('inf')  # Asignar un valor alto si no se pudo entrenar\n",
    "\n",
    "# print('============================================================')\n",
    "# # Imprimir los resultados de validación cruzada\n",
    "# print(\"\\nResultados de Validación Cruzada (MAE promedio):\")\n",
    "# for opt, mae in results.items():\n",
    "#     print(f\"{opt}: {mae:.4f}\")\n",
    "# print('============================================================')\n",
    "# # Seleccionar el mejor optimizador\n",
    "# best_optimizer_name = min(results, key=results.get)\n",
    "# print(f\"\\nMejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "# print('============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor optimizador seleccionado: Adam\n"
     ]
    }
   ],
   "source": [
    "# CREATE THE MODEL AND\n",
    "# best_optimizer_name = min(results, key=results.get)\n",
    "best_optimizer_name = 'Adam'\n",
    "print(f\"Mejor optimizador seleccionado: {best_optimizer_name}\")\n",
    "\n",
    "model = build_model(optimizer_name=best_optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - loss: 31.7477 - mae: 0.8892 - val_loss: 26.7001 - val_mae: 0.4189 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25.7727 - mae: 0.5528 - val_loss: 21.8943 - val_mae: 0.3468 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.9915 - mae: 0.4515 - val_loss: 17.5639 - val_mae: 0.3238 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.7748 - mae: 0.4193 - val_loss: 13.7691 - val_mae: 0.2863 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.1720 - mae: 0.3835 - val_loss: 10.6731 - val_mae: 0.2700 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1787 - mae: 0.3532 - val_loss: 8.1509 - val_mae: 0.2543 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7236 - mae: 0.3199 - val_loss: 6.2125 - val_mae: 0.2501 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9587 - mae: 0.3317 - val_loss: 4.7704 - val_mae: 0.2498 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6475 - mae: 0.3210 - val_loss: 3.7277 - val_mae: 0.2577 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7147 - mae: 0.3543 - val_loss: 2.9424 - val_mae: 0.2392 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9457 - mae: 0.3173 - val_loss: 2.3959 - val_mae: 0.2458 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4444 - mae: 0.3321 - val_loss: 1.9840 - val_mae: 0.2434 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0598 - mae: 0.3525 - val_loss: 1.6894 - val_mae: 0.2549 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7575 - mae: 0.3431 - val_loss: 1.4411 - val_mae: 0.2531 - learning_rate: 0.0010\n",
      "Epoch 15/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6267 - mae: 0.3438 - val_loss: 1.2657 - val_mae: 0.2515 - learning_rate: 0.0010\n",
      "Epoch 16/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2798 - mae: 0.3098 - val_loss: 1.1089 - val_mae: 0.2426 - learning_rate: 0.0010\n",
      "Epoch 17/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1676 - mae: 0.3148 - val_loss: 0.9889 - val_mae: 0.2462 - learning_rate: 0.0010\n",
      "Epoch 18/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1885 - mae: 0.3586 - val_loss: 0.9060 - val_mae: 0.2401 - learning_rate: 0.0010\n",
      "Epoch 19/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0506 - mae: 0.3443 - val_loss: 0.8395 - val_mae: 0.2520 - learning_rate: 0.0010\n",
      "Epoch 20/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0344 - mae: 0.3761 - val_loss: 0.7764 - val_mae: 0.2414 - learning_rate: 0.0010\n",
      "Epoch 21/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9231 - mae: 0.3542 - val_loss: 0.7213 - val_mae: 0.2443 - learning_rate: 0.0010\n",
      "Epoch 22/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8931 - mae: 0.3642 - val_loss: 0.6940 - val_mae: 0.2479 - learning_rate: 0.0010\n",
      "Epoch 23/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8902 - mae: 0.3494 - val_loss: 0.6777 - val_mae: 0.2503 - learning_rate: 0.0010\n",
      "Epoch 24/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7985 - mae: 0.3400 - val_loss: 0.6254 - val_mae: 0.2343 - learning_rate: 0.0010\n",
      "Epoch 25/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9066 - mae: 0.3739 - val_loss: 0.6474 - val_mae: 0.2741 - learning_rate: 0.0010\n",
      "Epoch 26/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7732 - mae: 0.3634 - val_loss: 0.5844 - val_mae: 0.2398 - learning_rate: 0.0010\n",
      "Epoch 27/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7751 - mae: 0.3736 - val_loss: 0.5714 - val_mae: 0.2452 - learning_rate: 0.0010\n",
      "Epoch 28/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7628 - mae: 0.3697 - val_loss: 0.5562 - val_mae: 0.2534 - learning_rate: 0.0010\n",
      "Epoch 29/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7018 - mae: 0.3490 - val_loss: 0.5430 - val_mae: 0.2588 - learning_rate: 0.0010\n",
      "Epoch 30/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6238 - mae: 0.3255 - val_loss: 0.5204 - val_mae: 0.2460 - learning_rate: 0.0010\n",
      "Epoch 31/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7015 - mae: 0.3553 - val_loss: 0.5276 - val_mae: 0.2536 - learning_rate: 0.0010\n",
      "Epoch 32/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6403 - mae: 0.3365 - val_loss: 0.5060 - val_mae: 0.2523 - learning_rate: 0.0010\n",
      "Epoch 33/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6541 - mae: 0.3570 - val_loss: 0.4859 - val_mae: 0.2444 - learning_rate: 0.0010\n",
      "Epoch 34/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6292 - mae: 0.3378 - val_loss: 0.4964 - val_mae: 0.2582 - learning_rate: 0.0010\n",
      "Epoch 35/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6169 - mae: 0.3457 - val_loss: 0.4938 - val_mae: 0.2636 - learning_rate: 0.0010\n",
      "Epoch 36/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6257 - mae: 0.3541 - val_loss: 0.4772 - val_mae: 0.2469 - learning_rate: 0.0010\n",
      "Epoch 37/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6886 - mae: 0.3599 - val_loss: 0.4744 - val_mae: 0.2428 - learning_rate: 0.0010\n",
      "Epoch 38/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6122 - mae: 0.3560 - val_loss: 0.4600 - val_mae: 0.2412 - learning_rate: 0.0010\n",
      "Epoch 39/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6436 - mae: 0.3671 - val_loss: 0.4475 - val_mae: 0.2412 - learning_rate: 0.0010\n",
      "Epoch 40/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6418 - mae: 0.3629 - val_loss: 0.4548 - val_mae: 0.2453 - learning_rate: 0.0010\n",
      "Epoch 41/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6083 - mae: 0.3504 - val_loss: 0.4411 - val_mae: 0.2438 - learning_rate: 0.0010\n",
      "Epoch 42/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5395 - mae: 0.3364 - val_loss: 0.4459 - val_mae: 0.2566 - learning_rate: 0.0010\n",
      "Epoch 43/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5731 - mae: 0.3350 - val_loss: 0.4559 - val_mae: 0.2441 - learning_rate: 0.0010\n",
      "Epoch 44/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5804 - mae: 0.3424 - val_loss: 0.4519 - val_mae: 0.2398 - learning_rate: 0.0010\n",
      "Epoch 45/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5712 - mae: 0.3406 - val_loss: 0.4372 - val_mae: 0.2442 - learning_rate: 0.0010\n",
      "Epoch 46/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5949 - mae: 0.3613 - val_loss: 0.4458 - val_mae: 0.2432 - learning_rate: 0.0010\n",
      "Epoch 47/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6468 - mae: 0.3723 - val_loss: 0.4402 - val_mae: 0.2422 - learning_rate: 0.0010\n",
      "Epoch 48/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6778 - mae: 0.3572 - val_loss: 0.4564 - val_mae: 0.2457 - learning_rate: 0.0010\n",
      "Epoch 49/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6417 - mae: 0.3624 - val_loss: 0.4480 - val_mae: 0.2500 - learning_rate: 0.0010\n",
      "Epoch 50/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6040 - mae: 0.3672 - val_loss: 0.4327 - val_mae: 0.2462 - learning_rate: 0.0010\n",
      "Epoch 51/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6585 - mae: 0.3722 - val_loss: 0.4556 - val_mae: 0.2569 - learning_rate: 0.0010\n",
      "Epoch 52/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5921 - mae: 0.3744 - val_loss: 0.4474 - val_mae: 0.2474 - learning_rate: 0.0010\n",
      "Epoch 53/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5961 - mae: 0.3577 - val_loss: 0.4433 - val_mae: 0.2512 - learning_rate: 0.0010\n",
      "Epoch 54/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5500 - mae: 0.3442 - val_loss: 0.4459 - val_mae: 0.2410 - learning_rate: 0.0010\n",
      "Epoch 55/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4739 - mae: 0.2919\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5914 - mae: 0.3565 - val_loss: 0.4435 - val_mae: 0.2483 - learning_rate: 0.0010\n",
      "Epoch 56/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6162 - mae: 0.3506 - val_loss: 0.4064 - val_mae: 0.2360 - learning_rate: 5.0000e-04\n",
      "Epoch 57/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5356 - mae: 0.3309 - val_loss: 0.4047 - val_mae: 0.2402 - learning_rate: 5.0000e-04\n",
      "Epoch 58/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5411 - mae: 0.3401 - val_loss: 0.4114 - val_mae: 0.2503 - learning_rate: 5.0000e-04\n",
      "Epoch 59/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5418 - mae: 0.3472 - val_loss: 0.3945 - val_mae: 0.2339 - learning_rate: 5.0000e-04\n",
      "Epoch 60/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5633 - mae: 0.3692 - val_loss: 0.4007 - val_mae: 0.2370 - learning_rate: 5.0000e-04\n",
      "Epoch 61/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5296 - mae: 0.3314 - val_loss: 0.3932 - val_mae: 0.2331 - learning_rate: 5.0000e-04\n",
      "Epoch 62/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5490 - mae: 0.3630 - val_loss: 0.3994 - val_mae: 0.2346 - learning_rate: 5.0000e-04\n",
      "Epoch 63/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4938 - mae: 0.3188 - val_loss: 0.4014 - val_mae: 0.2363 - learning_rate: 5.0000e-04\n",
      "Epoch 64/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5704 - mae: 0.3531 - val_loss: 0.3930 - val_mae: 0.2344 - learning_rate: 5.0000e-04\n",
      "Epoch 65/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5145 - mae: 0.3263 - val_loss: 0.3930 - val_mae: 0.2359 - learning_rate: 5.0000e-04\n",
      "Epoch 66/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5496 - mae: 0.3460 - val_loss: 0.4003 - val_mae: 0.2391 - learning_rate: 5.0000e-04\n",
      "Epoch 67/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5896 - mae: 0.3510 - val_loss: 0.3903 - val_mae: 0.2274 - learning_rate: 5.0000e-04\n",
      "Epoch 68/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5933 - mae: 0.3431 - val_loss: 0.3951 - val_mae: 0.2285 - learning_rate: 5.0000e-04\n",
      "Epoch 69/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5214 - mae: 0.3445 - val_loss: 0.4042 - val_mae: 0.2372 - learning_rate: 5.0000e-04\n",
      "Epoch 70/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5679 - mae: 0.3429 - val_loss: 0.4004 - val_mae: 0.2377 - learning_rate: 5.0000e-04\n",
      "Epoch 71/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5975 - mae: 0.3390 - val_loss: 0.4006 - val_mae: 0.2351 - learning_rate: 5.0000e-04\n",
      "Epoch 72/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4652 - mae: 0.2947\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5019 - mae: 0.3381 - val_loss: 0.4051 - val_mae: 0.2413 - learning_rate: 5.0000e-04\n",
      "Epoch 73/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5405 - mae: 0.3483 - val_loss: 0.3784 - val_mae: 0.2299 - learning_rate: 2.5000e-04\n",
      "Epoch 74/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4657 - mae: 0.3300 - val_loss: 0.3766 - val_mae: 0.2311 - learning_rate: 2.5000e-04\n",
      "Epoch 75/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5727 - mae: 0.3681 - val_loss: 0.3745 - val_mae: 0.2297 - learning_rate: 2.5000e-04\n",
      "Epoch 76/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5219 - mae: 0.3623 - val_loss: 0.3726 - val_mae: 0.2290 - learning_rate: 2.5000e-04\n",
      "Epoch 77/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4868 - mae: 0.3259 - val_loss: 0.3716 - val_mae: 0.2274 - learning_rate: 2.5000e-04\n",
      "Epoch 78/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4507 - mae: 0.3204 - val_loss: 0.3706 - val_mae: 0.2284 - learning_rate: 2.5000e-04\n",
      "Epoch 79/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5804 - mae: 0.3620 - val_loss: 0.3720 - val_mae: 0.2289 - learning_rate: 2.5000e-04\n",
      "Epoch 80/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5350 - mae: 0.3489 - val_loss: 0.3804 - val_mae: 0.2315 - learning_rate: 2.5000e-04\n",
      "Epoch 81/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4932 - mae: 0.3468 - val_loss: 0.3756 - val_mae: 0.2307 - learning_rate: 2.5000e-04\n",
      "Epoch 82/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4893 - mae: 0.3186 - val_loss: 0.3747 - val_mae: 0.2290 - learning_rate: 2.5000e-04\n",
      "Epoch 83/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4329 - mae: 0.3263\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5058 - mae: 0.3311 - val_loss: 0.3777 - val_mae: 0.2302 - learning_rate: 2.5000e-04\n",
      "Epoch 84/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5414 - mae: 0.3573 - val_loss: 0.3696 - val_mae: 0.2304 - learning_rate: 1.2500e-04\n",
      "Epoch 85/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5236 - mae: 0.3463 - val_loss: 0.3692 - val_mae: 0.2318 - learning_rate: 1.2500e-04\n",
      "Epoch 86/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5239 - mae: 0.3487 - val_loss: 0.3663 - val_mae: 0.2294 - learning_rate: 1.2500e-04\n",
      "Epoch 87/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5562 - mae: 0.3575 - val_loss: 0.3652 - val_mae: 0.2283 - learning_rate: 1.2500e-04\n",
      "Epoch 88/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4979 - mae: 0.3477 - val_loss: 0.3654 - val_mae: 0.2299 - learning_rate: 1.2500e-04\n",
      "Epoch 89/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5293 - mae: 0.3382 - val_loss: 0.3646 - val_mae: 0.2295 - learning_rate: 1.2500e-04\n",
      "Epoch 90/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4929 - mae: 0.3495 - val_loss: 0.3651 - val_mae: 0.2291 - learning_rate: 1.2500e-04\n",
      "Epoch 91/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4957 - mae: 0.3427 - val_loss: 0.3643 - val_mae: 0.2289 - learning_rate: 1.2500e-04\n",
      "Epoch 92/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4969 - mae: 0.3455 - val_loss: 0.3611 - val_mae: 0.2268 - learning_rate: 1.2500e-04\n",
      "Epoch 93/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5407 - mae: 0.3679 - val_loss: 0.3629 - val_mae: 0.2289 - learning_rate: 1.2500e-04\n",
      "Epoch 94/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5005 - mae: 0.3553 - val_loss: 0.3638 - val_mae: 0.2290 - learning_rate: 1.2500e-04\n",
      "Epoch 95/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5107 - mae: 0.3447 - val_loss: 0.3682 - val_mae: 0.2289 - learning_rate: 1.2500e-04\n",
      "Epoch 96/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4989 - mae: 0.3301 - val_loss: 0.3634 - val_mae: 0.2272 - learning_rate: 1.2500e-04\n",
      "Epoch 97/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5167 - mae: 0.4766\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6197 - mae: 0.3521 - val_loss: 0.3664 - val_mae: 0.2282 - learning_rate: 1.2500e-04\n",
      "Epoch 98/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4949 - mae: 0.3430 - val_loss: 0.3607 - val_mae: 0.2275 - learning_rate: 6.2500e-05\n",
      "Epoch 99/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5368 - mae: 0.3486 - val_loss: 0.3596 - val_mae: 0.2270 - learning_rate: 6.2500e-05\n",
      "Epoch 100/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5387 - mae: 0.3642 - val_loss: 0.3579 - val_mae: 0.2267 - learning_rate: 6.2500e-05\n",
      "Epoch 101/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4753 - mae: 0.3246 - val_loss: 0.3598 - val_mae: 0.2284 - learning_rate: 6.2500e-05\n",
      "Epoch 102/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5232 - mae: 0.3421 - val_loss: 0.3603 - val_mae: 0.2293 - learning_rate: 6.2500e-05\n",
      "Epoch 103/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4771 - mae: 0.3284 - val_loss: 0.3600 - val_mae: 0.2299 - learning_rate: 6.2500e-05\n",
      "Epoch 104/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5234 - mae: 0.3631 - val_loss: 0.3597 - val_mae: 0.2297 - learning_rate: 6.2500e-05\n",
      "Epoch 105/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4274 - mae: 0.3327\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4469 - mae: 0.3320 - val_loss: 0.3597 - val_mae: 0.2296 - learning_rate: 6.2500e-05\n",
      "Epoch 106/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5254 - mae: 0.3456 - val_loss: 0.3582 - val_mae: 0.2295 - learning_rate: 3.1250e-05\n",
      "Epoch 107/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5587 - mae: 0.3439 - val_loss: 0.3571 - val_mae: 0.2290 - learning_rate: 3.1250e-05\n",
      "Epoch 108/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5398 - mae: 0.3503 - val_loss: 0.3574 - val_mae: 0.2293 - learning_rate: 3.1250e-05\n",
      "Epoch 109/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5399 - mae: 0.3547 - val_loss: 0.3574 - val_mae: 0.2295 - learning_rate: 3.1250e-05\n",
      "Epoch 110/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5661 - mae: 0.3654 - val_loss: 0.3571 - val_mae: 0.2289 - learning_rate: 3.1250e-05\n",
      "Epoch 111/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4727 - mae: 0.3279 - val_loss: 0.3568 - val_mae: 0.2290 - learning_rate: 3.1250e-05\n",
      "Epoch 112/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4891 - mae: 0.3391 - val_loss: 0.3574 - val_mae: 0.2297 - learning_rate: 3.1250e-05\n",
      "Epoch 113/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5358 - mae: 0.3551 - val_loss: 0.3569 - val_mae: 0.2298 - learning_rate: 3.1250e-05\n",
      "Epoch 114/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4655 - mae: 0.3313 - val_loss: 0.3568 - val_mae: 0.2300 - learning_rate: 3.1250e-05\n",
      "Epoch 115/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5133 - mae: 0.3524 - val_loss: 0.3564 - val_mae: 0.2294 - learning_rate: 3.1250e-05\n",
      "Epoch 116/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5315 - mae: 0.3528 - val_loss: 0.3566 - val_mae: 0.2292 - learning_rate: 3.1250e-05\n",
      "Epoch 117/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5317 - mae: 0.3477 - val_loss: 0.3557 - val_mae: 0.2290 - learning_rate: 3.1250e-05\n",
      "Epoch 118/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5024 - mae: 0.3304 - val_loss: 0.3562 - val_mae: 0.2291 - learning_rate: 3.1250e-05\n",
      "Epoch 119/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6351 - mae: 0.3846 - val_loss: 0.3550 - val_mae: 0.2280 - learning_rate: 3.1250e-05\n",
      "Epoch 120/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5605 - mae: 0.3559 - val_loss: 0.3545 - val_mae: 0.2279 - learning_rate: 3.1250e-05\n",
      "Epoch 121/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5315 - mae: 0.3509 - val_loss: 0.3546 - val_mae: 0.2280 - learning_rate: 3.1250e-05\n",
      "Epoch 122/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5162 - mae: 0.3421 - val_loss: 0.3548 - val_mae: 0.2284 - learning_rate: 3.1250e-05\n",
      "Epoch 123/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4915 - mae: 0.3346 - val_loss: 0.3548 - val_mae: 0.2289 - learning_rate: 3.1250e-05\n",
      "Epoch 124/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4574 - mae: 0.3272 - val_loss: 0.3542 - val_mae: 0.2284 - learning_rate: 3.1250e-05\n",
      "Epoch 125/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5101 - mae: 0.3256 - val_loss: 0.3556 - val_mae: 0.2291 - learning_rate: 3.1250e-05\n",
      "Epoch 126/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5144 - mae: 0.3460 - val_loss: 0.3548 - val_mae: 0.2288 - learning_rate: 3.1250e-05\n",
      "Epoch 127/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4717 - mae: 0.3293 - val_loss: 0.3540 - val_mae: 0.2285 - learning_rate: 3.1250e-05\n",
      "Epoch 128/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4810 - mae: 0.3436 - val_loss: 0.3538 - val_mae: 0.2285 - learning_rate: 3.1250e-05\n",
      "Epoch 129/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4859 - mae: 0.3399 - val_loss: 0.3529 - val_mae: 0.2286 - learning_rate: 3.1250e-05\n",
      "Epoch 130/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4983 - mae: 0.3403 - val_loss: 0.3525 - val_mae: 0.2285 - learning_rate: 3.1250e-05\n",
      "Epoch 131/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4581 - mae: 0.3274 - val_loss: 0.3524 - val_mae: 0.2283 - learning_rate: 3.1250e-05\n",
      "Epoch 132/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4757 - mae: 0.3256 - val_loss: 0.3535 - val_mae: 0.2291 - learning_rate: 3.1250e-05\n",
      "Epoch 133/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4793 - mae: 0.3320 - val_loss: 0.3526 - val_mae: 0.2288 - learning_rate: 3.1250e-05\n",
      "Epoch 134/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5068 - mae: 0.3561 - val_loss: 0.3524 - val_mae: 0.2286 - learning_rate: 3.1250e-05\n",
      "Epoch 135/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4683 - mae: 0.3249 - val_loss: 0.3518 - val_mae: 0.2284 - learning_rate: 3.1250e-05\n",
      "Epoch 136/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4696 - mae: 0.3352 - val_loss: 0.3521 - val_mae: 0.2283 - learning_rate: 3.1250e-05\n",
      "Epoch 137/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5063 - mae: 0.3567 - val_loss: 0.3519 - val_mae: 0.2285 - learning_rate: 3.1250e-05\n",
      "Epoch 138/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4937 - mae: 0.3287 - val_loss: 0.3515 - val_mae: 0.2281 - learning_rate: 3.1250e-05\n",
      "Epoch 139/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4605 - mae: 0.3273 - val_loss: 0.3515 - val_mae: 0.2283 - learning_rate: 3.1250e-05\n",
      "Epoch 140/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4679 - mae: 0.3177 - val_loss: 0.3519 - val_mae: 0.2286 - learning_rate: 3.1250e-05\n",
      "Epoch 141/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4749 - mae: 0.3301 - val_loss: 0.3530 - val_mae: 0.2290 - learning_rate: 3.1250e-05\n",
      "Epoch 142/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5135 - mae: 0.3397 - val_loss: 0.3529 - val_mae: 0.2289 - learning_rate: 3.1250e-05\n",
      "Epoch 143/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5101 - mae: 0.3792\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5199 - mae: 0.3612 - val_loss: 0.3531 - val_mae: 0.2293 - learning_rate: 3.1250e-05\n",
      "Epoch 144/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5502 - mae: 0.3541 - val_loss: 0.3525 - val_mae: 0.2294 - learning_rate: 1.5625e-05\n",
      "Epoch 145/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5576 - mae: 0.3490 - val_loss: 0.3523 - val_mae: 0.2294 - learning_rate: 1.5625e-05\n",
      "Epoch 146/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5797 - mae: 0.3591 - val_loss: 0.3522 - val_mae: 0.2293 - learning_rate: 1.5625e-05\n",
      "Epoch 147/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5033 - mae: 0.3536 - val_loss: 0.3514 - val_mae: 0.2287 - learning_rate: 1.5625e-05\n",
      "Epoch 148/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4498 - mae: 0.3200 - val_loss: 0.3514 - val_mae: 0.2287 - learning_rate: 1.5625e-05\n",
      "Epoch 149/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4790 - mae: 0.3156 - val_loss: 0.3515 - val_mae: 0.2288 - learning_rate: 1.5625e-05\n",
      "Epoch 150/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5927 - mae: 0.3464 - val_loss: 0.3514 - val_mae: 0.2287 - learning_rate: 1.5625e-05\n",
      "Epoch 151/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4841 - mae: 0.3261 - val_loss: 0.3513 - val_mae: 0.2286 - learning_rate: 1.5625e-05\n",
      "Epoch 152/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5146 - mae: 0.3347 - val_loss: 0.3513 - val_mae: 0.2288 - learning_rate: 1.5625e-05\n",
      "Epoch 153/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6115 - mae: 0.4059\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4857 - mae: 0.3427 - val_loss: 0.3513 - val_mae: 0.2288 - learning_rate: 1.5625e-05\n",
      "Epoch 154/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5246 - mae: 0.3430 - val_loss: 0.3511 - val_mae: 0.2291 - learning_rate: 7.8125e-06\n",
      "Epoch 155/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5198 - mae: 0.3431 - val_loss: 0.3518 - val_mae: 0.2293 - learning_rate: 7.8125e-06\n",
      "Epoch 156/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4788 - mae: 0.3485 - val_loss: 0.3517 - val_mae: 0.2293 - learning_rate: 7.8125e-06\n",
      "Epoch 157/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5002 - mae: 0.3344 - val_loss: 0.3516 - val_mae: 0.2293 - learning_rate: 7.8125e-06\n",
      "Epoch 158/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4805 - mae: 0.3380 - val_loss: 0.3517 - val_mae: 0.2294 - learning_rate: 7.8125e-06\n",
      "Epoch 159/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6263 - mae: 0.4297\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5779 - mae: 0.3630 - val_loss: 0.3517 - val_mae: 0.2295 - learning_rate: 7.8125e-06\n",
      "Epoch 160/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5070 - mae: 0.3521 - val_loss: 0.3514 - val_mae: 0.2293 - learning_rate: 3.9063e-06\n",
      "Epoch 161/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5231 - mae: 0.3580 - val_loss: 0.3514 - val_mae: 0.2294 - learning_rate: 3.9063e-06\n",
      "Epoch 162/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5105 - mae: 0.3478 - val_loss: 0.3515 - val_mae: 0.2295 - learning_rate: 3.9063e-06\n",
      "Epoch 163/250\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4796 - mae: 0.3372 - val_loss: 0.3513 - val_mae: 0.2294 - learning_rate: 3.9063e-06\n",
      "Epoch 164/250\n",
      "\u001b[1m 1/30\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3873 - mae: 0.3242\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4536 - mae: 0.3220 - val_loss: 0.3512 - val_mae: 0.2294 - learning_rate: 3.9063e-06\n"
     ]
    }
   ],
   "source": [
    "# TRAIN IT\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=250, batch_size=32,\n",
    "                    validation_split=0.2, verbose=1, callbacks=[\n",
    "                        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
    "                        # Puedes agregar más callbacks como TensorBoard si es necesario\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "============================================================\n",
      "Métricas del Modelo en el Conjunto de Prueba:\n",
      "Mean Absolute Error (MAE): 22245.86\n",
      "Root Mean Squared Error (RMSE): 38911.02\n",
      "Root Mean Squared Error (RMSE en %): 21.76%\n",
      "R-squared (R2): 0.8026\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Invertir la escala para obtener las predicciones en la escala original\n",
    "y_pred = price_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = price_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Calcular las métricas\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "mean_y_true = np.mean(y_true)\n",
    "rmse_percentage = (rmse / mean_y_true) * 100\n",
    "\n",
    "print(\"============================================================\")\n",
    "print(\"Métricas del Modelo en el Conjunto de Prueba:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "# print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE en %): {rmse_percentage:.2f}%\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "print(\"============================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
